{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # visualizations and charts\n",
    "import nltk\n",
    "import pprint\n",
    "from nltk.chunk import ne_chunk\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train-balanced-sarcasm.csv')\n",
    "empty_comments = data[\"comment\"].isna()\n",
    "empty_comments = data[empty_comments].index\n",
    "data.drop(empty_comments, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### how many comments do you want to process WARNING can take some time\n",
    "comments_0 = data[data['label']==0]['comment']\n",
    "comments_1 = data[data['label']==1]['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_label(comments, label):\n",
    "    processed = []\n",
    "    for comment in comments:\n",
    "        processed.append([comment, label])\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_nerc(comments, label):\n",
    "    processed = []\n",
    "    with tqdm(total=len(comments), file=sys.stdout) as pbar:    \n",
    "        for i, comment in enumerate(comments):\n",
    "            tokenize = nltk.word_tokenize(comment)\n",
    "            postag = nltk.pos_tag(tokenize)\n",
    "            tagged_nerc = ne_chunk(postag)\n",
    "            processed.append([tagged_nerc, label])\n",
    "            \n",
    "            pbar.set_description('processed: %d' % (1 + i))\n",
    "            pbar.update(1)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_0 = comment_label(comments_0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 505405: 100%|█████████████████████████████████████████████████████| 505405/505405 [3:20:26<00:00, 42.02it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-35449bcce041>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnerc_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomment_nerc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomments_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfile1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nerc_0.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w+\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfile1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnerc_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Close opend file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: write() argument must be str, not list"
     ]
    }
   ],
   "source": [
    "nerc_0 = comment_nerc(comments_0, 0)\n",
    "#error is fixed but i'm not running this again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"nerc_0.txt\", \"w+\")\n",
    "for item in nerc_0:\n",
    "    file1.write(\"%s\\n\" % item)\n",
    "    \n",
    "# Close opend file\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Tree('S', [('NC', 'NNP'), ('and', 'CC'), Tree('ORGANIZATION', [('NH', 'NNP')]), ('.', '.')]), 0], [Tree('S', [('You', 'PRP'), ('do', 'VBP'), ('know', 'VB'), ('west', 'JJS'), ('teams', 'NNS'), ('play', 'VBP'), ('against', 'IN'), ('west', 'JJ'), ('teams', 'NNS'), ('more', 'RBR'), ('than', 'IN'), ('east', 'JJ'), ('teams', 'NNS'), ('right', 'RB'), ('?', '.')]), 0]]\n"
     ]
    }
   ],
   "source": [
    "print(nerc_0[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_1 = comment_label(comments_1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 505368: 100%|████████████████████████████████████████████████████| 505368/505368 [11:46:12<00:00, 11.93it/s]\n"
     ]
    }
   ],
   "source": [
    "nerc_1 = comment_nerc(comments_1, 1)\n",
    "file2 = open(\"nerc_1.txt\", \"w+\")\n",
    "for item in nerc_1:\n",
    "    file2.write(\"%s\\n\" % item)\n",
    "    \n",
    "# Close opend file\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('You', 'PRP')\n",
      "('do', 'VBP')\n",
      "('know', 'VB')\n",
      "('west', 'JJS')\n",
      "('teams', 'NNS')\n",
      "('play', 'VBP')\n",
      "('against', 'IN')\n",
      "('west', 'JJ')\n",
      "('teams', 'NNS')\n",
      "('more', 'RBR')\n",
      "('than', 'IN')\n",
      "('east', 'JJ')\n",
      "('teams', 'NNS')\n",
      "('right', 'RB')\n",
      "('?', '.')\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for something in nerc_0[1]:\n",
    "    if something== int:\n",
    "        print(something)\n",
    "    else:\n",
    "        for part in something: \n",
    "            \n",
    "            print(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nerc0 = open('nerc_0.txt')\n",
    "nerc1 = open('nerc_1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_0 = []\n",
    "for item in nerc0:\n",
    "    list_0.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 = []\n",
    "for item in nerc1:\n",
    "    list_1.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('S', [('Because', 'IN'), ('they', 'PRP'), (\"'re\", 'VBP'), ('not', 'RB'), ('real', 'JJ'), ('human', 'JJ'), ('beings', 'NNS'), ('with', 'IN'), ('actual', 'JJ'), ('feelings', 'NNS'), (',', ','), ('according', 'VBG'), ('to', 'TO'), ('the', 'DT'), ('videos', 'NN'), ('put', 'VBD'), ('together', 'RB'), ('by', 'IN'), ('the', 'DT'), Tree('ORGANIZATION', [('GB', 'NNP')]), ('...', ':'), ('...', ':'), ('.', '.')]), 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(list_0[30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 505405: 100%|████████████████████████████████████████████████████| 505405/505405 [14:42:39<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "entities_0 = []\n",
    "with tqdm(total=len(comments_0), file=sys.stdout) as pbar: \n",
    "    for i, comment in enumerate(comments_0):\n",
    "        for sent in nltk.sent_tokenize(comment):\n",
    "             for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "                if hasattr(chunk, 'label'):\n",
    "                    entities_0.append(chunk.label() +' ' + ' '.join(c[0] for c in chunk))\n",
    "        \n",
    "        pbar.set_description('processed: %d' % (1 + i))\n",
    "        pbar.update(1)\n",
    "        \n",
    "print(entities_0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 845:   0%|                                                           | 844/505368 [00:22<4:17:39, 32.63it/s]"
     ]
    }
   ],
   "source": [
    "entities_1 = []\n",
    "with tqdm(total=len(comments_1), file=sys.stdout) as pbar: \n",
    "    for i, comment in enumerate(comments_1):\n",
    "        for sent in nltk.sent_tokenize(comment):\n",
    "             for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "                if hasattr(chunk, 'label'):\n",
    "                    entities_1.append(chunk.label() +' ' + ' '.join(c[0] for c in chunk))\n",
    "        \n",
    "        pbar.set_description('processed: %d' % (1 + i))\n",
    "        pbar.update(1)\n",
    "\n",
    "print(entities_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file3 = open(\"extract_0.txt\", \"w+\")\n",
    "for item in entities_0:\n",
    "    file3.write(\"%s\\n\" % item)\n",
    "    \n",
    "# Close opend file\n",
    "file3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file4 = open(\"extract_1.txt\", \"w+\")\n",
    "for item in entities_1:\n",
    "    file4.write(\"%s\\n\" % item)\n",
    "    \n",
    "# Close opend file\n",
    "file4.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
