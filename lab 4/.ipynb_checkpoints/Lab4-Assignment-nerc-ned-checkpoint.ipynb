{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4-Assignment about Named Entity Recognition, Classification and Disambiguation\n",
    "\n",
    "This notebook describes the assignment of Lab 4 of the text mining course. \n",
    "\n",
    "**Learning goals**\n",
    "* going from linguistic input format to representing it in a feature space\n",
    "* working with pretrained word embeddings\n",
    "* train a supervised classifier (SVM)\n",
    "* evaluate a supervised classifier (SVM)\n",
    "* perform feature ablation and gain insight into the contribution of various features\n",
    "* Learn how to evaluate an entity linking system.\n",
    "* Learn how to run two entity linking systems (AIDA and DBpedia Spotlight).\n",
    "* Learn how to interpret the system output and the evaluation results.\n",
    "* Get insight into differences between the two systems.\n",
    "* Be able to describe differences between the two methods in terms of their results.\n",
    "* Be able to propose future improvements based on the observed results.\n",
    "* Get insight into the difficulty of NED and how this depends on specific entity mentions.\n",
    "\n",
    "The assignment consists of 2 parts:\n",
    "\n",
    "* Named Entity Recornition and Classificaiton: excersizes 1 & 2\n",
    "* Named Entity Disambiguation and Linking: excersizes 3 & 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This notebook was originally created by [Marten Postma](https://martenpostma.github.io) and [Filip Ilievski](http://ilievski.nl) and dapated by Piek vossen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition and Classification\n",
    "\n",
    "Excercises 2 and 3 focus on Named Entity Recognition and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 18] Exercise 1 (NERC): Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  -a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 elements from the training instances feautures:\n",
      " [{'words': 'EU', 'pos': 'NNP'}, {'words': 'rejects', 'pos': 'VBZ'}, {'words': 'German', 'pos': 'JJ'}, {'words': 'call', 'pos': 'NN'}, {'words': 'to', 'pos': 'TO'}, {'words': 'boycott', 'pos': 'VB'}, {'words': 'British', 'pos': 'JJ'}, {'words': 'lamb', 'pos': 'NN'}, {'words': '.', 'pos': '.'}, {'words': 'Peter', 'pos': 'NNP'}]\n",
      "\n",
      "First 10 elements from the training instances NERC labels:\n",
      " ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-PER']\n"
     ]
    }
   ],
   "source": [
    "train = ConllCorpusReader('nerc_datasets/CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {\n",
    "        #features:\n",
    "        'words': token,\n",
    "        'pos': pos\n",
    "    }\n",
    "    training_features.append(a_dict)\n",
    "    training_gold_labels.append(ne_label)\n",
    "print('First 10 elements from the training instances feautures:\\n',training_features[:10])\n",
    "print()\n",
    "print('First 10 elements from the training instances NERC labels:\\n', training_gold_labels[:10])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 elements from the test instances feautures:\n",
      " [{'words': 'SOCCER', 'pos': 'NN'}, {'words': '-', 'pos': ':'}, {'words': 'JAPAN', 'pos': 'NNP'}, {'words': 'GET', 'pos': 'VB'}, {'words': 'LUCKY', 'pos': 'NNP'}, {'words': 'WIN', 'pos': 'NNP'}, {'words': ',', 'pos': ','}, {'words': 'CHINA', 'pos': 'NNP'}, {'words': 'IN', 'pos': 'IN'}, {'words': 'SURPRISE', 'pos': 'DT'}]\n",
      "\n",
      "First 10 elements from the test instances NERC labels:\n",
      " ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "### Adapt the path to point to the NERC_datasets folder on your local machine\n",
    "test = ConllCorpusReader('nerc_datasets/CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    a_dict = {\n",
    "        #features:\n",
    "        'words':token,\n",
    "        'pos': pos\n",
    "    }\n",
    "    test_features.append(a_dict)\n",
    "    test_gold_labels.append(ne_label)\n",
    "print('First 10 elements from the test instances feautures:\\n',test_features[:10])\n",
    "print()\n",
    "print('First 10 elements from the test instances NERC labels:\\n', test_gold_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test?\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter \n",
    "# my_list=[1,2,1,3,2,5]\n",
    "# Counter(my_list)\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m How many instances are in train and test?\u001b[0m\n",
      "There are 203621 instances in train and 46435 instances in test.\n",
      "\n",
      "\u001b[1m NERC-label frequency distribution of train: \u001b[0m  \n",
      "         frequency\n",
      "O          169578\n",
      "B-LOC        7140\n",
      "B-PER        6600\n",
      "B-ORG        6321\n",
      "I-PER        4528\n",
      "I-ORG        3704\n",
      "B-MISC       3438\n",
      "I-LOC        1157\n",
      "I-MISC       1155\n",
      "\n",
      "\u001b[1m NERC-label frequency distribution of test: \u001b[0m \n",
      "         frequency\n",
      "O           38323\n",
      "B-LOC        1668\n",
      "B-ORG        1661\n",
      "B-PER        1617\n",
      "I-PER        1156\n",
      "I-ORG         835\n",
      "B-MISC        702\n",
      "I-LOC         257\n",
      "I-MISC        216\n",
      "\n",
      "\u001b[1m Balance and differences between test and train : \u001b[0m \n",
      "The data is reasonably balanced, in both the train and test data thehighest frequency is the O NERC label, and the lowest frequency is the I-MISC NERC label.The only difference is the frequency of the B-LOC NERC label,the train data has relatively more instances.\n"
     ]
    }
   ],
   "source": [
    "print( '\\033[1m How many instances are in train and test?\\033[0m')\n",
    "print('There are %d instances in train and %d instances in test.'%(len(training_features),len(test_features)))\n",
    "print()\n",
    "\n",
    "#Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "df_train = pandas.DataFrame(training_gold_labels)\n",
    "df_train.columns = ['frequency']\n",
    "print('\\033[1m NERC-label frequency distribution of train: \\033[0m  \\n', df_train.apply(pandas.value_counts))\n",
    "print()\n",
    "df_test = pandas.DataFrame(test_gold_labels)\n",
    "df_test.columns = ['frequency']\n",
    "print('\\033[1m NERC-label frequency distribution of test: \\033[0m \\n', df_test.apply(pandas.value_counts))\n",
    "print()\n",
    "print('\\033[1m Balance and differences between test and train : \\033[0m \\n\\\n",
    "The data is reasonably balanced, in both the train and test data the\\\n",
    "highest frequency is the O NERC label, and the lowest frequency is the I-MISC NERC label.\\\n",
    "The only difference is the frequency of the B-LOC NERC label,\\\n",
    "the train data has relatively more instances.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] c) Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n",
    "\n",
    "Tip: You’ve concatenated train and test into one list and then you’ve applied the DictVectorizer.\n",
    "The order of the rows is maintained. You can hence use an index (number of training instances) to split the_array back into train and test. Do NOT use: `\n",
    "from sklearn.model_selection import train_test_split` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (250056, 27361) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-cf855ea8ce29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mall_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_features\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mthe_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnew_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthe_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m203621\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnew_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthe_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m203621\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output array must be C or F contiguous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate array with shape (250056, 27361) and data type float64"
     ]
    }
   ],
   "source": [
    "vec = DictVectorizer()\n",
    "all_features = training_features + test_features\n",
    "the_array = vec.fit_transform(all_features).toarray()\n",
    "new_train = the_array[:203621]\n",
    "new_test = the_array[203621:]\n",
    "print(new_train)\n",
    "print(new_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] d) Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report).**\n",
    "The train (*lin_clf.fit*) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.81      0.78      0.79      1668\n",
      "      B-MISC       0.78      0.66      0.72       702\n",
      "       B-ORG       0.79      0.52      0.63      1661\n",
      "       B-PER       0.86      0.44      0.58      1617\n",
      "       I-LOC       0.62      0.53      0.57       257\n",
      "      I-MISC       0.57      0.59      0.58       216\n",
      "       I-ORG       0.70      0.47      0.56       835\n",
      "       I-PER       0.33      0.87      0.48      1156\n",
      "           O       0.98      0.98      0.98     38323\n",
      "\n",
      "    accuracy                           0.92     46435\n",
      "   macro avg       0.72      0.65      0.65     46435\n",
      "weighted avg       0.94      0.92      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lin_clf.fit(new_train, training_gold_labels)\n",
    "predict_label = lin_clf.predict(new_test)\n",
    "print(classification_report(test_gold_labels, predict_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2d)** The outside tags perform well, this is probably because there is not much overlap/ no connection between named entities and the locations, verbs, prepositions, etc.. \n",
    "The inside person tags performs poorly, probably because of last names that also accur on their own and are therefor classified wrongly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6 points] e) Train a model that uses the embeddings of these words as inputs. Test again on the same data as in 2d. Generate a classification report and compare the results with the classifier you built in 2d.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('model/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_train=[]\n",
    "embedding_gold_labels=[]\n",
    "embedding_test=[]\n",
    "embedding_gold_test_labels=[]\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    \n",
    "    if token!='' and token!='DOCSTART':\n",
    "        if token in word_embedding_model:\n",
    "            vector=word_embedding_model[token]\n",
    "        else:\n",
    "            vector=[0]*300\n",
    "        embedding_train.append(vector)\n",
    "        embedding_gold_labels.append(ne_label)\n",
    "        \n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    \n",
    "    if token!='' and token!='DOCSTART':\n",
    "        if token in word_embedding_model:\n",
    "            vector=word_embedding_model[token]\n",
    "        else:\n",
    "            vector=[0]*300\n",
    "        embedding_test.append(vector)\n",
    "        embedding_gold_test_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.76      0.80      0.78      1668\n",
      "      B-MISC       0.72      0.70      0.71       702\n",
      "       B-ORG       0.69      0.64      0.66      1661\n",
      "       B-PER       0.75      0.67      0.71      1617\n",
      "       I-LOC       0.51      0.42      0.46       257\n",
      "      I-MISC       0.60      0.54      0.57       216\n",
      "       I-ORG       0.48      0.33      0.39       835\n",
      "       I-PER       0.59      0.50      0.54      1156\n",
      "           O       0.97      0.99      0.98     38323\n",
      "\n",
      "    accuracy                           0.93     46435\n",
      "   macro avg       0.68      0.62      0.64     46435\n",
      "weighted avg       0.92      0.93      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lin_clf.fit(embedding_train, embedding_gold_labels)\n",
    "predict_label = lin_clf.predict(embedding_test)\n",
    "print(classification_report(embedding_gold_test_labels, predict_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2e) Comparison classification reports 2d and 2e**:\n",
    "Overall the precision is a bit lower when using the embeddings as input, however, the difference is not great and the accuracy, macro avg and weighted avd are still good. The only exeption is the inside person tag, when using the embeddings of the words as input it performs much better. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 10] Exercise 2 (NERC): feature inspection using the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\n",
    "**[6 points] a. Perform the same steps as in the previous exercise. Make sure you end up for both the training part (*df_train*) and the test part (*df_test*) with:**\n",
    "* the features representation using **DictVectorizer**\n",
    "* the NERC labels in a list\n",
    "\n",
    "Please note that this is the same setup as in the previous exercise:\n",
    "* load both train and test using:\n",
    "    * list of dictionaries for features\n",
    "    * list of NERC labels\n",
    "* combine train and test features in a list and represent them using one hot encoding\n",
    "* train using the training features and NERC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    }
   ],
   "source": [
    "##### Adapt the path to point to your local copy of NERC_datasets\n",
    "path = 'nerc_datasets/kaggle/ner_v2.csv'\n",
    "kaggle_dataset = pandas.read_csv(path, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050795"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 20000\n"
     ]
    }
   ],
   "source": [
    "df_train = kaggle_dataset[:100000]\n",
    "df_test = kaggle_dataset[100000:120000]\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "\n",
    "training2_features = []\n",
    "training2_gold_labels = []\n",
    "test2_features = []\n",
    "test2_gold_labels = []\n",
    "for index, instance in df_train.iterrows():\n",
    "    a_dict = {\n",
    "        'id': index,\n",
    "        'lemma':instance['lemma'],\n",
    "        'pos':instance['pos'],\n",
    "        'shape':instance['shape'],\n",
    "        'word':instance['word'],\n",
    "        'next-lemma': instance['next-lemma'],\n",
    "        'next-pos': instance['next-pos'],\n",
    "        'next-shape': instance['next-shape'],\n",
    "        'next-word': instance['next-word'],\n",
    "        'next-next-lemma': instance['next-next-lemma'],\n",
    "        'next-next-pos': instance['next-next-pos'],\n",
    "        'next-next-shape': instance['next-next-shape'],\n",
    "        'next-next-word': instance['next-next-word'],\n",
    "        'prev-iob':instance['prev-iob'],\n",
    "        'prev-lemma':instance['prev-lemma'],\n",
    "        'prev-pos':instance['prev-pos'],\n",
    "        'prev-shape':instance['prev-shape'],\n",
    "        'prev-word':instance['prev-word'],\n",
    "        'prev-prev-iob':instance['prev-prev-iob'],\n",
    "        'prev-prev-lemma':instance['prev-prev-lemma'],\n",
    "        'prev-prev-pos':instance['prev-prev-pos'],\n",
    "        'prev-prev-shape':instance['prev-prev-shape'],\n",
    "        'prev-prev-word':instance['prev-prev-word'],\n",
    "        'sentence_idx':instance['sentence_idx']   \n",
    "    }\n",
    "    training2_features.append(a_dict)\n",
    "    training2_gold_labels.append(instance['tag'])\n",
    "    \n",
    "for index, instance in df_test.iterrows():\n",
    "    a_dict = {\n",
    "        'id': index,\n",
    "        'lemma':instance['lemma'],\n",
    "        'pos':instance['pos'],\n",
    "        'shape':instance['shape'],\n",
    "        'word':instance['word'],\n",
    "        'next-lemma': instance['next-lemma'],\n",
    "        'next-pos': instance['next-pos'],\n",
    "        'next-shape': instance['next-shape'],\n",
    "        'next-word': instance['next-word'],\n",
    "        'next-next-lemma': instance['next-next-lemma'],\n",
    "        'next-next-pos': instance['next-next-pos'],\n",
    "        'next-next-shape': instance['next-next-shape'],\n",
    "        'next-next-word': instance['next-next-word'],\n",
    "        'prev-iob':instance['prev-iob'],\n",
    "        'prev-lemma':instance['prev-lemma'],\n",
    "        'prev-pos':instance['prev-pos'],\n",
    "        'prev-shape':instance['prev-shape'],\n",
    "        'prev-word':instance['prev-word'],\n",
    "        'prev-prev-iob':instance['prev-prev-iob'],\n",
    "        'prev-prev-lemma':instance['prev-prev-lemma'],\n",
    "        'prev-prev-pos':instance['prev-prev-pos'],\n",
    "        'prev-prev-shape':instance['prev-prev-shape'],\n",
    "        'prev-prev-word':instance['prev-prev-word'],\n",
    "        'sentence_idx':instance['sentence_idx']   \n",
    "    }\n",
    "    test2_features.append(a_dict)\n",
    "    test2_gold_labels.append(instance['tag'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m First  5 training feature: \u001b[0m [{'id': 0, 'lemma': 'thousand', 'pos': 'NNS', 'shape': 'capitalized', 'word': 'Thousands', 'next-lemma': 'of', 'next-pos': 'IN', 'next-shape': 'lowercase', 'next-word': 'of', 'next-next-lemma': 'demonstr', 'next-next-pos': 'NNS', 'next-next-shape': 'lowercase', 'next-next-word': 'demonstrators', 'prev-iob': '__START1__', 'prev-lemma': '__start1__', 'prev-pos': '__START1__', 'prev-shape': 'wildcard', 'prev-word': '__START1__', 'prev-prev-iob': '__START2__', 'prev-prev-lemma': '__start2__', 'prev-prev-pos': '__START2__', 'prev-prev-shape': 'wildcard', 'prev-prev-word': '__START2__', 'sentence_idx': 1.0}, {'id': 1, 'lemma': 'of', 'pos': 'IN', 'shape': 'lowercase', 'word': 'of', 'next-lemma': 'demonstr', 'next-pos': 'NNS', 'next-shape': 'lowercase', 'next-word': 'demonstrators', 'next-next-lemma': 'have', 'next-next-pos': 'VBP', 'next-next-shape': 'lowercase', 'next-next-word': 'have', 'prev-iob': 'O', 'prev-lemma': 'thousand', 'prev-pos': 'NNS', 'prev-shape': 'capitalized', 'prev-word': 'Thousands', 'prev-prev-iob': '__START1__', 'prev-prev-lemma': '__start1__', 'prev-prev-pos': '__START1__', 'prev-prev-shape': 'wildcard', 'prev-prev-word': '__START1__', 'sentence_idx': 1.0}, {'id': 2, 'lemma': 'demonstr', 'pos': 'NNS', 'shape': 'lowercase', 'word': 'demonstrators', 'next-lemma': 'have', 'next-pos': 'VBP', 'next-shape': 'lowercase', 'next-word': 'have', 'next-next-lemma': 'march', 'next-next-pos': 'VBN', 'next-next-shape': 'lowercase', 'next-next-word': 'marched', 'prev-iob': 'O', 'prev-lemma': 'of', 'prev-pos': 'IN', 'prev-shape': 'lowercase', 'prev-word': 'of', 'prev-prev-iob': 'O', 'prev-prev-lemma': 'thousand', 'prev-prev-pos': 'NNS', 'prev-prev-shape': 'capitalized', 'prev-prev-word': 'Thousands', 'sentence_idx': 1.0}, {'id': 3, 'lemma': 'have', 'pos': 'VBP', 'shape': 'lowercase', 'word': 'have', 'next-lemma': 'march', 'next-pos': 'VBN', 'next-shape': 'lowercase', 'next-word': 'marched', 'next-next-lemma': 'through', 'next-next-pos': 'IN', 'next-next-shape': 'lowercase', 'next-next-word': 'through', 'prev-iob': 'O', 'prev-lemma': 'demonstr', 'prev-pos': 'NNS', 'prev-shape': 'lowercase', 'prev-word': 'demonstrators', 'prev-prev-iob': 'O', 'prev-prev-lemma': 'of', 'prev-prev-pos': 'IN', 'prev-prev-shape': 'lowercase', 'prev-prev-word': 'of', 'sentence_idx': 1.0}, {'id': 4, 'lemma': 'march', 'pos': 'VBN', 'shape': 'lowercase', 'word': 'marched', 'next-lemma': 'through', 'next-pos': 'IN', 'next-shape': 'lowercase', 'next-word': 'through', 'next-next-lemma': 'london', 'next-next-pos': 'NNP', 'next-next-shape': 'capitalized', 'next-next-word': 'London', 'prev-iob': 'O', 'prev-lemma': 'have', 'prev-pos': 'VBP', 'prev-shape': 'lowercase', 'prev-word': 'have', 'prev-prev-iob': 'O', 'prev-prev-lemma': 'demonstr', 'prev-prev-pos': 'NNS', 'prev-prev-shape': 'lowercase', 'prev-prev-word': 'demonstrators', 'sentence_idx': 1.0}]\n",
      " \u001b[1mand the training NERC labels: \u001b[0m ['O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\u001b[1mFirst 5 test feature: \u001b[0m [{'id': 0, 'lemma': 'thousand', 'pos': 'NNS', 'shape': 'capitalized', 'word': 'Thousands', 'next-lemma': 'of', 'next-pos': 'IN', 'next-shape': 'lowercase', 'next-word': 'of', 'next-next-lemma': 'demonstr', 'next-next-pos': 'NNS', 'next-next-shape': 'lowercase', 'next-next-word': 'demonstrators', 'prev-iob': '__START1__', 'prev-lemma': '__start1__', 'prev-pos': '__START1__', 'prev-shape': 'wildcard', 'prev-word': '__START1__', 'prev-prev-iob': '__START2__', 'prev-prev-lemma': '__start2__', 'prev-prev-pos': '__START2__', 'prev-prev-shape': 'wildcard', 'prev-prev-word': '__START2__', 'sentence_idx': 1.0}, {'id': 1, 'lemma': 'of', 'pos': 'IN', 'shape': 'lowercase', 'word': 'of', 'next-lemma': 'demonstr', 'next-pos': 'NNS', 'next-shape': 'lowercase', 'next-word': 'demonstrators', 'next-next-lemma': 'have', 'next-next-pos': 'VBP', 'next-next-shape': 'lowercase', 'next-next-word': 'have', 'prev-iob': 'O', 'prev-lemma': 'thousand', 'prev-pos': 'NNS', 'prev-shape': 'capitalized', 'prev-word': 'Thousands', 'prev-prev-iob': '__START1__', 'prev-prev-lemma': '__start1__', 'prev-prev-pos': '__START1__', 'prev-prev-shape': 'wildcard', 'prev-prev-word': '__START1__', 'sentence_idx': 1.0}, {'id': 2, 'lemma': 'demonstr', 'pos': 'NNS', 'shape': 'lowercase', 'word': 'demonstrators', 'next-lemma': 'have', 'next-pos': 'VBP', 'next-shape': 'lowercase', 'next-word': 'have', 'next-next-lemma': 'march', 'next-next-pos': 'VBN', 'next-next-shape': 'lowercase', 'next-next-word': 'marched', 'prev-iob': 'O', 'prev-lemma': 'of', 'prev-pos': 'IN', 'prev-shape': 'lowercase', 'prev-word': 'of', 'prev-prev-iob': 'O', 'prev-prev-lemma': 'thousand', 'prev-prev-pos': 'NNS', 'prev-prev-shape': 'capitalized', 'prev-prev-word': 'Thousands', 'sentence_idx': 1.0}, {'id': 3, 'lemma': 'have', 'pos': 'VBP', 'shape': 'lowercase', 'word': 'have', 'next-lemma': 'march', 'next-pos': 'VBN', 'next-shape': 'lowercase', 'next-word': 'marched', 'next-next-lemma': 'through', 'next-next-pos': 'IN', 'next-next-shape': 'lowercase', 'next-next-word': 'through', 'prev-iob': 'O', 'prev-lemma': 'demonstr', 'prev-pos': 'NNS', 'prev-shape': 'lowercase', 'prev-word': 'demonstrators', 'prev-prev-iob': 'O', 'prev-prev-lemma': 'of', 'prev-prev-pos': 'IN', 'prev-prev-shape': 'lowercase', 'prev-prev-word': 'of', 'sentence_idx': 1.0}, {'id': 4, 'lemma': 'march', 'pos': 'VBN', 'shape': 'lowercase', 'word': 'marched', 'next-lemma': 'through', 'next-pos': 'IN', 'next-shape': 'lowercase', 'next-word': 'through', 'next-next-lemma': 'london', 'next-next-pos': 'NNP', 'next-next-shape': 'capitalized', 'next-next-word': 'London', 'prev-iob': 'O', 'prev-lemma': 'have', 'prev-pos': 'VBP', 'prev-shape': 'lowercase', 'prev-word': 'have', 'prev-prev-iob': 'O', 'prev-prev-lemma': 'demonstr', 'prev-prev-pos': 'NNS', 'prev-prev-shape': 'lowercase', 'prev-prev-word': 'demonstrators', 'sentence_idx': 1.0}]\n",
      " \u001b[1mand the test NERC labels:\u001b[0m ['O', 'O', 'O', 'B-geo', 'I-geo']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1m First  5 training feature: \\033[0m\", training2_features[:5])\n",
    "print(\" \\033[1mand the training NERC labels: \\033[0m\", training2_gold_labels[:5] )\n",
    "print()\n",
    "print(\"\\033[1mFirst 5 test feature: \\033[0m\", training2_features[:5])\n",
    "print(\" \\033[1mand the test NERC labels:\\033[0m\", test2_gold_labels[:5] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (100000, 90530) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-4d34981553c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining2_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest2_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output array must be C or F contiguous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate array with shape (100000, 90530) and data type float64"
     ]
    }
   ],
   "source": [
    "train2 = vec.fit_transform(training2_features[:]).toarray()\n",
    "test2 = vec.fit_transform(test2_features).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine train and test features in a list and represent them using one hot encoding\n",
    "all_features = training2_features[:5] + test2_features[:5]\n",
    "combined_features = vec.fit_transform(all_features).toarray()\n",
    "print(\"\\033[1m first 5 train and test features combined: \\033[0m \\n\", combined_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] b. Train and evaluate the model and provide the classification report:**\n",
    "* use the SVM to predict NERC labels on the test data\n",
    "* evaluate the performance of the SVM on the test data\n",
    "\n",
    "Analyze the performance per NERC label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Linking\n",
    "\n",
    "Excersizes 3 and 4 focus on Entity linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excersize 3 (NEL): Quantitative analysis  [Points: 15] \n",
    "\n",
    "In this assignment, you are going to work with two systems for entity linking: AIDA and DBpedia Spotlight. You will run them on an entity linking dataset and evaluate their performance. You will perform both quantitative and qualitative analysis of their output, and run one of these systems on your own text. We will reflect on the results of these tasks.\n",
    "\n",
    "**Note:** We will use the dataset Reuters-128 in this assignment. This dataset was introduced in the notebook 'Lab4.3-Entity-linking-tools', so you probably have it already (in case you do not have it make sure you download it from Canvas first and put it in the same location as this notebook). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1a** Write code that runs both systems on the full Reuters-128 dataset. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both systems on the full Reuters-128 dataset\n",
    "from rdflib import Graph, URIRef\n",
    "from tqdm import tqdm \n",
    "import sys\n",
    "import requests\n",
    "import urllib\n",
    "import urllib.parse\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.parse import urlencode\n",
    "import xml.etree.cElementTree as ET\n",
    "from lxml import etree\n",
    "import time\n",
    "import json\n",
    "\n",
    "# import our own utility functions and classes\n",
    "import lab4_utils as utils\n",
    "import lab4_classes as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aida_disambiguation_url = \"https://gate.d5.mpi-inf.mpg.de/aida/service/disambiguate\"\n",
    "spotlight_disambiguation_url=\"http://model.dbpedia-spotlight.org/en/disambiguate\"\n",
    "\n",
    "def aida_disambiguation(articles, aida_url):\n",
    "\n",
    "    with tqdm(total=len(articles), file=sys.stdout) as pbar: #use of progress bar\n",
    "        for i, article in enumerate(articles):\n",
    "            \n",
    "            #premark entities in text\n",
    "            original_content = article.content \n",
    "            new_content=original_content       \n",
    "            for entity in reversed(article.entity_mentions):\n",
    "                entity_span=new_content[entity.begin_index: entity.end_index]\n",
    "                new_content=new_content[:entity.begin_index] + '[[' + entity_span + ']]' + new_content[entity.end_index:]\n",
    "\n",
    "            # Send request to aida library\n",
    "            params={\"text\": new_content, \"tag_mode\": 'manual'}\n",
    "            request = Request(aida_url, urlencode(params).encode())\n",
    "            this_json = urlopen(request).read().decode('unicode-escape')\n",
    "            try:\n",
    "                results=json.loads(this_json)\n",
    "            except:\n",
    "                continue\n",
    "           \n",
    "            #normalize and clean the json response of aida\n",
    "            dis_entities={}\n",
    "            for dis_entity in results['mentions']:\n",
    "                if 'bestEntity' in dis_entity.keys():\n",
    "                    best_entity=dis_entity['bestEntity']['kbIdentifier']\n",
    "                    clean_url=best_entity[5:] #SKIP YAGO:\n",
    "                else:\n",
    "                    clean_url='NIL'\n",
    "                dis_entities[str(dis_entity['offset'])] = clean_url \n",
    "                \n",
    "            # store the entity with link\n",
    "            for entity in article.entity_mentions:\n",
    "                start = entity.begin_index\n",
    "                try:\n",
    "                    dis_url = str(dis_entities[str(start)])  \n",
    "                except:\n",
    "                    dis_url='NIL'\n",
    "                entity.aida_link = dis_url  \n",
    "\n",
    "            # update progress bar\n",
    "            pbar.set_description('processed: %d' % (1 + i))\n",
    "            pbar.update(1)\n",
    "    return articles\n",
    "\n",
    "def spotlight_disambiguate(articles, spotlight_url):\n",
    "    \n",
    "    with tqdm(total=len(articles), file=sys.stdout) as pbar:\n",
    "        for i, article in enumerate(articles):\n",
    "           \n",
    "            # initiate xml structure\n",
    "            annotation = etree.Element(\"annotation\", text=article.content)\n",
    "            \n",
    "            # create surface form elements of the article data for our xml \n",
    "            for mention in article.entity_mentions:\n",
    "                sf = etree.SubElement(annotation, \"surfaceForm\")\n",
    "                sf.set(\"name\", mention.mention)\n",
    "                sf.set(\"offset\", str(mention.begin_index))\n",
    "            my_xml=etree.tostring(annotation, xml_declaration=True, encoding='UTF-8')\n",
    "            \n",
    "            # send request to spotlight and process json response\n",
    "            results=requests.post(spotlight_url, urllib.parse.urlencode({'text':my_xml, 'confidence': 0.5}), \n",
    "                                  headers={'Accept': 'application/json'})\n",
    "            \n",
    "            j=results.json()\n",
    "            dis_entities={}\n",
    "            if 'Resources' in j: \n",
    "                resources=j['Resources']\n",
    "            else: \n",
    "                resources=[]\n",
    "            for dis_entity in resources:\n",
    "                dis_entities[str(dis_entity['@offset'])] = utils.normalizeURL(dis_entity['@URI'])\n",
    "            \n",
    "            # store spotlight link for the article\n",
    "            for entity in article.entity_mentions:\n",
    "                start = entity.begin_index\n",
    "                if str(start) in dis_entities:\n",
    "                    dis_url = dis_entities[str(start)]\n",
    "                else:\n",
    "                    dis_url = 'NIL'\n",
    "                entity.spotlight_link = dis_url\n",
    "    \n",
    "            # update progress bar\n",
    "            pbar.set_description('processed: %d' % (1 + i))\n",
    "            pbar.update(1)\n",
    "                \n",
    "            # Pause for 1s to prevent overloading the server\n",
    "            time.sleep(1)\n",
    "    return articles\n",
    "\n",
    "def process_both(article_set):\n",
    "    processed_aida= aida_disambiguation(article_set, aida_disambiguation_url)\n",
    "    processed_spotlight=spotlight_disambiguate(processed_aida, spotlight_disambiguation_url)\n",
    "    return processed_spotlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_file='Reuters-128.ttl'\n",
    "articles=utils.load_article_from_nif_file(reuters_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run if server always overloads when passing all articles to spotlight\n",
    "set1 = articles[0:32]\n",
    "set2 = articles[32:64]\n",
    "set3 = articles[64:96]\n",
    "set4 = articles[96:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                   | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grietje001\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: invalid escape sequence '\\/'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 32:  97%|██████████████████████████▏| 31/32 [01:01<00:01,  1.97s/it]\n",
      "processed: 32: 100%|███████████████████████████| 32/32 [00:57<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "process1 = process_both(set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                   | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grietje001\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: invalid escape sequence '\\/'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 32:  94%|█████████████████████████▎ | 30/32 [01:10<00:04,  2.34s/it]\n",
      "processed: 32: 100%|███████████████████████████| 32/32 [00:58<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "process2 = process_both(set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                   | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grietje001\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: invalid escape sequence '\\/'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 32:  97%|██████████████████████████▏| 31/32 [00:27<00:00,  1.15it/s]\n",
      "processed: 32: 100%|███████████████████████████| 32/32 [00:56<00:00,  1.78s/it]\n"
     ]
    }
   ],
   "source": [
    "process3 = process_both(set3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 1:   3%|▉                            | 1/32 [00:00<00:05,  5.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grietje001\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: invalid escape sequence '\\/'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 32:  97%|██████████████████████████▏| 31/32 [00:45<00:01,  1.47s/it]\n",
      "processed: 32: 100%|███████████████████████████| 32/32 [00:58<00:00,  1.83s/it]\n"
     ]
    }
   ],
   "source": [
    "process4 = process_both(set4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_processed = process1 + process2 + process3 + process4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1b** Write code that evaluates the two systems on this dataset by computing their overall precision, recall, and F1-score. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to compute the precision, recall, and F1-score for each of the systems on this dataset\n",
    "def evaluate_entity_linking(system_decisions, gold_decisions):\n",
    "\n",
    "    tp=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "    \n",
    "    for gold_entity,system_entity in zip(gold_decisions,system_decisions):\n",
    "        if gold_entity=='NIL' and system_entity=='NIL': continue\n",
    "        if gold_entity==system_entity:\n",
    "            tp+=1\n",
    "        else:\n",
    "            if gold_entity!='NIL':\n",
    "                fn+=1\n",
    "            if system_entity!='NIL':\n",
    "                fp+=1\n",
    "\n",
    "    print('TP: %d; \\nFP: %d, \\nFN: %d' % (tp, fp, fn))            \n",
    "\n",
    "    precision=tp/(tp+fp)\n",
    "    recall=tp/(tp+fn)\n",
    "    f1=2*precision*recall/(precision+recall)\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_link = []\n",
    "aida_link = []\n",
    "spot_link = []\n",
    "\n",
    "for article in all_processed:\n",
    "    for mention in article.entity_mentions:\n",
    "        gold_link.append(mention.gold_link)\n",
    "        aida_link.append(mention.aida_link)\n",
    "        spot_link.append(mention.spotlight_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 297; \n",
      "FP: 183, \n",
      "FN: 353\n",
      "(0.61875, 0.45692307692307693, 0.5256637168141594)\n"
     ]
    }
   ],
   "source": [
    "aida_evaluate = evaluate_entity_linking(aida_link, gold_link)\n",
    "print(aida_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 300; \n",
      "FP: 228, \n",
      "FN: 350\n",
      "(0.5681818181818182, 0.46153846153846156, 0.5093378607809848)\n"
     ]
    }
   ],
   "source": [
    "spotlight_evaluate = evaluate_entity_linking(spot_link, gold_link)\n",
    "print(spotlight_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The acquired precision, recall and f1 score from the evaluations of spotlight and aida are presented in the table below\n",
    "```\n",
    "             precision             recall                 f1 \n",
    "     aida    0.61875               0.45692307692307693    0.5256637168141594\n",
    "spotlight    0.5681818181818182    0.46153846153846156    0.5093378607809848\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1c** What is the F1-score per system? Which system performs better? Is that also the better system in terms of precision and recall? Which is higher and what does that mean (hint: think of NIL entities)?(5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score for aida is 0.526 and for spotlight is 0.509, aida has a slightly higher performance than spotlight. \n",
    "In terms of precision aida also takes the lead, this is because the number of false positives is lower for aida. A false positive in this case would entail aida providing a link for the entity whilst the gold link is NIL. The precision is influenced by the number of false positives as it takes the number of true positives and divides it by true positives plus false positives.\n",
    "\n",
    "However, the recall for aida is slightly lower than the recall for spotlight. This is because the number of false negatives is lower for spotlight. A false negative here is when spotlight gives back a wrong link or NIL whilst the gold value is non-NIL link. The recall is affected by the number of false negatives as it divides the number of true positives by the number of true positives plus false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excersize 4 (NEL): Qualitative analysis [Points: 15] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2a** Check the entity disambiguation by AIDA against the gold entities on the document with identifier \"http://aksw.org/N3/Reuters-128/82#char=0,1370\" (write code to print the entity mentions, gold links and AIDA links). (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 1: 100%|██████████| 1/1 [00:04<00:00,  4.42s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maumau/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: DeprecationWarning: invalid escape sequence '\\/'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 1: 100%|██████████| 1/1 [00:02<00:00,  2.53s/it]\n",
      "Exchanges and telecommunications authorities should abolish their restrictions on full and free dissemination of information to the investment and banking communities, Reuters Holdings Plc RTRS.L chairman Sir Christopher Hogg said. In the 1986 annual repoprt, he said lengthy negotiations had brought agreement with the Tokyo and London Stock Exchanges for fuller, but still not complete, access to market data through Reuter services. Many other markets maintain restrictions, he added. Hogg said members of some markets appear to believe that information restrictions protected their interests. In other cases, exchanges seem to be limiting the distribution of data in order to provide competitive advantage to their own commercial information businesses. He also noted that despite increasing liberalisation in the telecommunications field, some countries continue to protect their state monopolies at the expense of other economic sectors. Reuter dealing services remain excluded from such countries. As a result, banking communities serving entire economies are put at a competitive disadvantage, he added. Reuters increased its 1986 pre-tax profit by 39 pct from the previous year to 130.1 mln stg on a 43 pct rise in revenues to 620.9 mln stg. Earnings per ordinary share were up 47 pct to 19.4p. The annual shareholder meeting will be held in London on April 29.\n",
      "http://aksw.org/N3/Reuters-128/82#char=0,1370\n",
      "|mention: Reuters Holdings Plc\t|gold:\tReuters\t|aida:\tReuters_Group\t|\n",
      "|mention: Christopher Hogg\t|gold:\tNIL\t|aida:\tNIL\t|\n",
      "|mention: Tokyo\t|gold:\tTokyo_Stock_Exchange\t|aida:\tTokyo\t|\n",
      "|mention: London Stock Exchanges\t|gold:\tLondon_Stock_Exchange\t|aida:\tNIL\t|\n",
      "|mention: Hogg\t|gold:\tNIL\t|aida:\tEdward_Hogg\t|\n",
      "|mention: Reuters\t|gold:\tReuters\t|aida:\tReuters\t|\n",
      "|mention: London\t|gold:\tLondon\t|aida:\tLondon\t|\n"
     ]
    }
   ],
   "source": [
    "for index, item in enumerate(articles):\n",
    "    if item.identifier == 'http://aksw.org/N3/Reuters-128/82#char=0,1370':\n",
    "        break\n",
    "else:\n",
    "    index = -1\n",
    "test_items=articles[index:index+1]\n",
    "processed_aida=aida_disambiguation(test_items, aida_disambiguation_url)\n",
    "processed_both=spotlight_disambiguate(processed_aida, spotlight_disambiguation_url)\n",
    "an_article=processed_both[0]\n",
    "doc_id=an_article.identifier\n",
    "print(an_article.content)\n",
    "print(doc_id)\n",
    "for m in an_article.entity_mentions:\n",
    "    print('|mention: %s\\t|gold:\\t%s\\t|aida:\\t%s\\t|' % (m.mention, m.gold_link, m.aida_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in this document that one of the mentions of \"Tokyo\" is disambiguated wrongly by AIDA as `Tokyo` (it should be `Tokyo_Stock_Exchange`). Knowing how AIDA works, what would be your explanation for this error? (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 2a\n",
    "AIDA uses two types of connections in their algorithm: \n",
    "\n",
    "*The first type is between a mention and an entity instance and tells us how often is an instance is referred to by a mention. \n",
    "*The second connection type is between two entity instances; it tells us how well-connected are two entities.\n",
    "\n",
    "When looking why Tokyo was classified as tokyo instead of Tokyo_Stock_Exchange. It is ovious that Tokyo \"the city\" is mentioned more often then Tokyo_Stock_Exchange. And when looking at the second type spotlight will have looked at london and Tokyo and seeing that these two city will be be well connected the algorithm will incorrectly assign Tokyo \"the city\" to Tokyo. # Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2b** Check the entity disambiguation by Spotlight against the gold entities on the document \"http://aksw.org/N3/Reuters-128/36#char=0,1146\" (write code to print the entity mentions, gold links and Spotlight links). (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 1: 100%|██████████| 1/1 [00:04<00:00,  4.54s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maumau/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: DeprecationWarning: invalid escape sequence '\\/'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 1: 100%|██████████| 1/1 [00:02<00:00,  2.26s/it]\n",
      "http://aksw.org/N3/Reuters-128/36#char=0,1146\n",
      "|mention: U.S. Treasury\t|gold:\tUnited_States_Department_of_the_Treasury\t|spotlight:\tUnited_States_Department_of_the_Treasury |\n",
      "|mention: Group of Five\t|gold:\tGroup_of_Five\t|spotlight:\tGroup_of_Five |\n",
      "|mention: Gerhard Stoltenberg\t|gold:\tGerhard_Stoltenberg\t|spotlight:\tGerhard_Stoltenberg |\n",
      "|mention: Bundesbank\t|gold:\tDeutsche_Bundesbank\t|spotlight:\tGerman_Federal_Bank |\n",
      "|mention: Karl Otto Poehl\t|gold:\tKarl_Otto_Pöhl\t|spotlight:\tNIL |\n",
      "|mention: Edouard Balladur\t|gold:\tÉdouard_Balladur\t|spotlight:\tÉdouard_Balladur |\n",
      "|mention: Jacques de Larosiere\t|gold:\tJacques_de_Larosière\t|spotlight:\tNIL |\n",
      "|mention: Kiichi Miyazawa\t|gold:\tKiichi_Miyazawa\t|spotlight:\tKiichi_Miyazawa |\n",
      "|mention: Satoshi Sumita\t|gold:\tSatoshi_Sumita\t|spotlight:\tNIL |\n",
      "|mention: Robin Leigh Pemberton\t|gold:\tRobin_Leigh-Pemberton,_Baron_Kingsdown\t|spotlight:\tNIL |\n",
      "|mention: Group of Seven\t|gold:\tG7\t|spotlight:\tGroup_of_Seven |\n",
      "|mention: Giovanni Goria\t|gold:\tGiovanni_Goria\t|spotlight:\tGiovanni_Goria |\n",
      "|mention: Treasury\t|gold:\tUnited_States_Department_of_the_Treasury\t|spotlight:\tHM_Treasury |\n",
      "|mention: James Baker\t|gold:\tJames_Baker\t|spotlight:\tJames_Baker |\n",
      "|mention: Baker\t|gold:\tJames_Baker\t|spotlight:\tJames_Baker |\n",
      "|mention: Goria\t|gold:\tGiovanni_Goria\t|spotlight:\tGiovanni_Goria |\n",
      "|mention: Group of Seven\t|gold:\tG7\t|spotlight:\tGroup_of_Seven |\n",
      "|mention: Paris\t|gold:\tParis\t|spotlight:\tParis |\n",
      "|mention: Italy\t|gold:\tItaly\t|spotlight:\tKingdom_of_Italy |\n"
     ]
    }
   ],
   "source": [
    "for index2, item in enumerate(articles):\n",
    "    if item.identifier == 'http://aksw.org/N3/Reuters-128/36#char=0,1146':\n",
    "        break\n",
    "else:\n",
    "    index2 = -1\n",
    "items2=articles[index2:index2+1]\n",
    "processed_aida2=aida_disambiguation(items2, aida_disambiguation_url)\n",
    "processed_both2=spotlight_disambiguate(processed_aida2, spotlight_disambiguation_url)\n",
    "an_article2=processed_both2[0]\n",
    "doc_id2=an_article2.identifier\n",
    "print(doc_id2)\n",
    "for m in an_article2.entity_mentions:\n",
    "    print('|mention: %s\\t|gold:\\t%s\\t|spotlight:\\t%s |' % (m.mention, m.gold_link, m.spotlight_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in this document that the mention of \"Group of Seven\" is disambiguated wrongly by Spotlight as `G8` (it should be `G7`). Knowing how Spotlight works, what would be your explanation for this error? (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anwser 2b  \n",
    "In the above table you see the gold and spotlight linking of the text http://aksw.org/N3/Reuters-128/36#char=0,1146. It is stated in the question that Spotlight  disambiguated wrongly Group of Seven as G8. But when we run the experment the resulting link was Group_of_Seven in both instances of the Group of Seven. In the rest of the table the results seems to be also correct so can't find the reason for this result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2c** In the document with identifier \"http://aksw.org/N3/Reuters-128/67#char=0,1627\":\n",
    "- both systems correctly decide that \"Michel Dufour\" is a `NIL` entity with no representation in the English Wikipedia. \n",
    "- however, Spotlight later decides that \"Dufour\" refers to `Guillaume-Henri_Dufour`\n",
    "\n",
    "How would you help Spotlight fix this error? (Hint: think of how you would know that \"Dufour\" is a NIL entity in that document) (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 1: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maumau/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: DeprecationWarning: invalid escape sequence '\\/'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 1: 100%|██████████| 1/1 [00:03<00:00,  3.34s/it]\n",
      "http://aksw.org/N3/Reuters-128/36#char=0,1146\n",
      "Top officials of leading industrial nations arrived at the U.S. Treasury main building to begin a meeting of the Group of Five. Officials seen arriving by Reuter correspondents included West German Finance Minister Gerhard Stoltenberg and Bundesbank President Karl Otto Poehl, French Finance Minister Edouard Balladur and his central banker Jacques de Larosiere. Also seen arriving were Japanese Finance Minister Kiichi Miyazawa and Japans central bank governor Satoshi Sumita and British Chancellor of the Exchequer and central bank governor Robin Leigh Pemberton. There was no immediate sign of Italian or Canadian officials. Monetary sources have said a fully blown meeting of the Group of Seven is expected to begin around 3 p.m. local time (1900 gmt) and last at least until 6 p.m. (2200 gmt), when a communique is expected to be issued. Italian sources said Italian acting Finance Minister Giovanni Goria met Treasury Secretary James Baker last night. At those talks Baker apparently convinced Goria, who declined to attend the February meeting of the Group of Seven in Paris, that Italy would participate fully in any meaningful decisions.\n",
      "|mention: U.S. Treasury\t|gold:\tUnited_States_Department_of_the_Treasury\t|spotlight:\tUnited_States_Department_of_the_Treasury |\n",
      "|mention: Group of Five\t|gold:\tGroup_of_Five\t|spotlight:\tGroup_of_Five |\n",
      "|mention: Gerhard Stoltenberg\t|gold:\tGerhard_Stoltenberg\t|spotlight:\tGerhard_Stoltenberg |\n",
      "|mention: Bundesbank\t|gold:\tDeutsche_Bundesbank\t|spotlight:\tGerman_Federal_Bank |\n",
      "|mention: Karl Otto Poehl\t|gold:\tKarl_Otto_Pöhl\t|spotlight:\tNIL |\n",
      "|mention: Edouard Balladur\t|gold:\tÉdouard_Balladur\t|spotlight:\tÉdouard_Balladur |\n",
      "|mention: Jacques de Larosiere\t|gold:\tJacques_de_Larosière\t|spotlight:\tNIL |\n",
      "|mention: Kiichi Miyazawa\t|gold:\tKiichi_Miyazawa\t|spotlight:\tKiichi_Miyazawa |\n",
      "|mention: Satoshi Sumita\t|gold:\tSatoshi_Sumita\t|spotlight:\tNIL |\n",
      "|mention: Robin Leigh Pemberton\t|gold:\tRobin_Leigh-Pemberton,_Baron_Kingsdown\t|spotlight:\tNIL |\n",
      "|mention: Group of Seven\t|gold:\tG7\t|spotlight:\tGroup_of_Seven |\n",
      "|mention: Giovanni Goria\t|gold:\tGiovanni_Goria\t|spotlight:\tGiovanni_Goria |\n",
      "|mention: Treasury\t|gold:\tUnited_States_Department_of_the_Treasury\t|spotlight:\tHM_Treasury |\n",
      "|mention: James Baker\t|gold:\tJames_Baker\t|spotlight:\tJames_Baker |\n",
      "|mention: Baker\t|gold:\tJames_Baker\t|spotlight:\tJames_Baker |\n",
      "|mention: Goria\t|gold:\tGiovanni_Goria\t|spotlight:\tGiovanni_Goria |\n",
      "|mention: Group of Seven\t|gold:\tG7\t|spotlight:\tGroup_of_Seven |\n",
      "|mention: Paris\t|gold:\tParis\t|spotlight:\tParis |\n",
      "|mention: Italy\t|gold:\tItaly\t|spotlight:\tKingdom_of_Italy |\n"
     ]
    }
   ],
   "source": [
    "for index3, item in enumerate(articles):\n",
    "    if item.identifier == 'http://aksw.org/N3/Reuters-128/67#char=0,1627':\n",
    "        break\n",
    "else:\n",
    "    index3 = -1\n",
    "    \n",
    "items3=articles[index3:index3+1]\n",
    "processed_aida3=aida_disambiguation(items3, aida_disambiguation_url)\n",
    "processed_both3=spotlight_disambiguate(processed_aida3, spotlight_disambiguation_url)\n",
    "\n",
    "an_article3=processed_both2[0]\n",
    "doc_id3=an_article3.identifier\n",
    "print(doc_id2)\n",
    "print(an_article3.content)\n",
    "for m in an_article3.entity_mentions:\n",
    "    print('|mention: %s\\t|gold:\\t%s\\t|spotlight:\\t%s |' % (m.mention, m.gold_link, m.spotlight_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# anwsers 2c \n",
    "The first time dufour is mentioned his first name is given so \n",
    "spotlight looks this full name up and can't find the person \n",
    "to connect it to and so labels it as a NIL. The second and third \n",
    "time dufour is mentioned only his surnamen is used. When spotligt\n",
    "looks this name up he does find a person to link to. This is the wrong \n",
    "person. This could be solved by making spotlight remender that the the \n",
    "dufour in the text is micheal dufour. And only add an another dufour if\n",
    "an other first name is given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
