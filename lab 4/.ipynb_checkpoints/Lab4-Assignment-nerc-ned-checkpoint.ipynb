{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4-Assignment about Named Entity Recognition, Classification and Disambiguation\n",
    "\n",
    "This notebook describes the assignment of Lab 4 of the text mining course. \n",
    "\n",
    "**Learning goals**\n",
    "* going from linguistic input format to representing it in a feature space\n",
    "* working with pretrained word embeddings\n",
    "* train a supervised classifier (SVM)\n",
    "* evaluate a supervised classifier (SVM)\n",
    "* perform feature ablation and gain insight into the contribution of various features\n",
    "* Learn how to evaluate an entity linking system.\n",
    "* Learn how to run two entity linking systems (AIDA and DBpedia Spotlight).\n",
    "* Learn how to interpret the system output and the evaluation results.\n",
    "* Get insight into differences between the two systems.\n",
    "* Be able to describe differences between the two methods in terms of their results.\n",
    "* Be able to propose future improvements based on the observed results.\n",
    "* Get insight into the difficulty of NED and how this depends on specific entity mentions.\n",
    "\n",
    "The assignment consists of 2 parts:\n",
    "\n",
    "* Named Entity Recornition and Classificaiton: excersizes 1 & 2\n",
    "* Named Entity Disambiguation and Linking: excersizes 3 & 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This notebook was originally created by [Marten Postma](https://martenpostma.github.io) and [Filip Ilievski](http://ilievski.nl) and dapated by Piek vossen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition and Classification\n",
    "\n",
    "Excercises 2 and 3 focus on Named Entity Recognition and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 18] Exercise 1 (NERC): Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  -a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "### Adapt the path to point to the nerc_datasets folder on your local machine\n",
    "train = ConllCorpusReader('/Users/piek/Desktop/ONDERWIJS/data/nerc_datasets/CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {\n",
    "       # add features\n",
    "    }\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adapt the path to point to the NERC_datasets folder on your local machine\n",
    "train = ConllCorpusReader('/Users/piek/Desktop/ONDERWIJS/nerc_datasets/CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {\n",
    "        # add features\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test?\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "\n",
    "my_list=[1,2,1,3,2,5]\n",
    "Counter(my_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] c) Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n",
    "\n",
    "Tip: You’ve concatenated train and test into one list and then you’ve applied the DictVectorizer.\n",
    "The order of the rows is maintained. You can hence use an index (number of training instances) to split the_array back into train and test. Do NOT use: `\n",
    "from sklearn.model_selection import train_test_split` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "the_array = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] d) Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report).**\n",
    "The train (*lin_clf.fit*) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### [ YOUR CODE SHOULD GO HERE ]\n",
    "# lin_clf.fit( # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6 points] e) Train a model that uses the embeddings of these words as inputs. Test again on the same data as in 2d. Generate a classification report and compare the results with the classifier you built in 2d.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 10] Exercise 2 (NERC): feature inspection using the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\n",
    "**[6 points] a. Perform the same steps as in the previous exercise. Make sure you end up for both the training part (*df_train*) and the test part (*df_test*) with:**\n",
    "* the features representation using **DictVectorizer**\n",
    "* the NERC labels in a list\n",
    "\n",
    "Please note that this is the same setup as in the previous exercise:\n",
    "* load both train and test using:\n",
    "    * list of dictionaries for features\n",
    "    * list of NERC labels\n",
    "* combine train and test features in a list and represent them using one hot encoding\n",
    "* train using the training features and NERC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Adapt the path to point to your local copy of NERC_datasets\n",
    "path = '/Users/piek/Desktop/ONDERWIJS/data/nerc_datasets/kaggle/ner_v2.csv'\n",
    "kaggle_dataset = pandas.read_csv(path, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = kaggle_dataset[:100000]\n",
    "df_test = kaggle_dataset[100000:120000]\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] b. Train and evaluate the model and provide the classification report:**\n",
    "* use the SVM to predict NERC labels on the test data\n",
    "* evaluate the performance of the SVM on the test data\n",
    "\n",
    "Analyze the performance per NERC label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Linking\n",
    "\n",
    "Excersizes 3 and 4 focus on Entity linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excersize 3 (NEL): Quantitative analysis  [Points: 15] \n",
    "\n",
    "In this assignment, you are going to work with two systems for entity linking: AIDA and DBpedia Spotlight. You will run them on an entity linking dataset and evaluate their performance. You will perform both quantitative and qualitative analysis of their output, and run one of these systems on your own text. We will reflect on the results of these tasks.\n",
    "\n",
    "**Note:** We will use the dataset Reuters-128 in this assignment. This dataset was introduced in the notebook 'Lab4.3-Entity-linking-tools', so you probably have it already (in case you do not have it make sure you download it from Canvas first and put it in the same location as this notebook). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1a** Write code that runs both systems on the full Reuters-128 dataset. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both systems on the full Reuters-128 dataset\n",
    "\n",
    "from rdflib import Graph, URIRef\n",
    "from tqdm import tqdm \n",
    "import sys\n",
    "import requests\n",
    "import urllib\n",
    "import urllib.parse\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.parse import urlencode\n",
    "import xml.etree.cElementTree as ET\n",
    "from lxml import etree\n",
    "import time\n",
    "import json\n",
    "\n",
    "# import our own utility functions and classes\n",
    "import lab4_utils as utils\n",
    "import lab4_classes as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_file='Reuters-128.ttl'\n",
    "articles=utils.load_article_from_nif_file(reuters_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aida_disambiguation(articles, aida_url):\n",
    "    \"\"\"\n",
    "    Perform disambiguation with AIDA.\n",
    "    \"\"\"\n",
    "    with tqdm(total=len(articles), file=sys.stdout) as pbar:  #pbar provides a nice progress bar for the interation over the articles\n",
    "        for i, article in enumerate(articles):\n",
    "                                    \n",
    "            # AIDA expects entity mentions that are pre-marked inside text. \n",
    "            # should be transformed to \"[[Obama]] visited [[Paris]] today.\"\n",
    "            original_content = article.content \n",
    "            new_content=original.content       \n",
    "            for entity in reversed(article.entity_mentions):\n",
    "                entity_span=new_content[entity.begin_index: entity.end_index]\n",
    "                new_content=new_content[:entity.begin_index] + '[[' + entity_span + ']]' + new_content[entity.end_index:]\n",
    "\n",
    "            # Now, we can run the AIDA library with this string.\n",
    "            params={\"text\": new_content, \"tag_mode\": 'manual'}\n",
    "            request = Request(aida_url, urlencode(params).encode())\n",
    "            # AIDA returns a json structure\n",
    "            this_json = urlopen(request).read().decode('unicode-escape')\n",
    "            try:\n",
    "                results=json.loads(this_json)\n",
    "            except:\n",
    "                continue\n",
    "            # print(this_json)\n",
    "            # Let's normalize the disambiguated entities.\n",
    "            # This means mostly removing the first part of the URI which is always the same (YAGO:)\n",
    "            # and leaving only the entity identification part (e.g., Barack_Obama).\n",
    "            dis_entities={}\n",
    "            # We iterate over the data elements \"mentions\" in the json results\n",
    "            for dis_entity in results['mentions']:\n",
    "               # print(dis_entity)\n",
    "                ## AIDI labels the bestEntity in the json\n",
    "                if 'bestEntity' in dis_entity.keys():\n",
    "                    best_entity=dis_entity['bestEntity']['kbIdentifier']\n",
    "                    clean_url=best_entity[5:] #SKIP YAGO:\n",
    "                else:\n",
    "                    clean_url='NIL'\n",
    "                dis_entities[str(dis_entity['offset'])] = clean_url # BECOMES THE VALUE IN THE DICTIONARY FOR THE OFFSET(REPRESENTING THE START OF THE MENTION) IN THE TEXT\n",
    "                \n",
    "            # We can now store the entity to our class instance for later processing.\n",
    "            for entity in article.entity_mentions:\n",
    "                start = entity.begin_index\n",
    "                try:\n",
    "                    dis_url = str(dis_entities[str(start)])  # WE GET THE DISAMBIGUATED URL\n",
    "                except:\n",
    "                    dis_url='NIL'\n",
    "                entity.aida_link = dis_url  # THE ENTITY IS ENRICHED WITH THE AIDA_LINK\n",
    "\n",
    "            # The next two lines only update the progress bar\n",
    "            pbar.set_description('processed: %d' % (1 + i))\n",
    "            pbar.update(1)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spotlight_disambiguate(articles, spotlight_url):\n",
    "    \"\"\"\n",
    "    Perform disambiguation with DBpedia Spotlight.\n",
    "    \"\"\"\n",
    "    with tqdm(total=len(articles), file=sys.stdout) as pbar:\n",
    "        for i, article in enumerate(articles):\n",
    "            # Similar as with AIDA, we first prepare the document text and the mentions\n",
    "            # in order to provide these to Spotlight as input.\n",
    "            \n",
    "            # We build up the XML structure that Spotligh wants as input\n",
    "            # The next function Element creates the XML element with the text attribute\n",
    "            annotation = etree.Element(\"annotation\", text=article.content)\n",
    "            \n",
    "            # We iterate over the eneity mentions from our Reuters data to create the surface form elements\n",
    "            for mention in article.entity_mentions:\n",
    "                sf = etree.SubElement(annotation, \"surfaceForm\")\n",
    "                sf.set(\"name\", mention.mention)\n",
    "                sf.set(\"offset\", str(mention.begin_index))\n",
    "            my_xml=etree.tostring(annotation, xml_declaration=True, encoding='UTF-8')\n",
    "            # Send a disambiguation request to spotlight\n",
    "            results=requests.post(spotlight_url, urllib.parse.urlencode({'text':my_xml, 'confidence': 0.5}), \n",
    "                                  headers={'Accept': 'application/json'})\n",
    "            # Note that you can adjust the confidence value. Check the online demo to see the effect. \n",
    "            # What will happen with the recall and precision if you increase the confidence?\n",
    "            \n",
    "            # Process the results and normalize the entity URIs\n",
    "            j=results.json()\n",
    "            dis_entities={}\n",
    "            if 'Resources' in j: \n",
    "                resources=j['Resources']\n",
    "            else: \n",
    "                resources=[]\n",
    "            for dis_entity in resources:\n",
    "                dis_entities[str(dis_entity['@offset'])] = utils.normalizeURL(dis_entity['@URI'])\n",
    "            \n",
    "            # Let's now store the URLs by Spotlight to our class for later analysis.\n",
    "            for entity in article.entity_mentions:\n",
    "                start = entity.begin_index\n",
    "                if str(start) in dis_entities:\n",
    "                    dis_url = dis_entities[str(start)]\n",
    "                else:\n",
    "                    dis_url = 'NIL'\n",
    "                entity.spotlight_link = dis_url\n",
    "    \n",
    "            # The next two lines only update the progress bar\n",
    "            pbar.set_description('processed: %d' % (1 + i))\n",
    "            pbar.update(1)\n",
    "                \n",
    "            # Pause for 100ms to prevent overloading the server\n",
    "            time.sleep(0.1)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_items = articles[0:5] #test initial code with something less heavy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aida_disambiguation_url = \"https://gate.d5.mpi-inf.mpg.de/aida/service/disambiguate\"\n",
    "spotlight_disambiguation_url=\"http://model.dbpedia-spotlight.org/en/disambiguate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 1:  20%|██████████████                                                        | 1/5 [00:00<00:02,  1.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: invalid escape sequence '\\/'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 5: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.82it/s]\n",
      "processed: 5: 100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "processed_aida=aida_disambiguation(test_items, aida_disambiguation_url)\n",
    "processed_both=spotlight_disambiguate(processed_aida, spotlight_disambiguation_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://aksw.org/N3/Reuters-128/34#char=0,912\n",
      "\n",
      "|mention: Social Affairs Ministry\t|gold:\tMinistry_of_Social_Affairs_and_Employment_(Netherlands)\t|aida:\tNIL\t|spotlight:\tNIL |\n",
      "http://aksw.org/N3/Reuters-128/70#char=0,282\n",
      "\n",
      "|mention: Federal Reserve\t|gold:\tFederal_Reserve_System\t|aida:\tNIL\t|spotlight:\tFederal_Reserve_System |\n",
      "|mention: Fed\t|gold:\tFederal_Reserve_System\t|aida:\tFederal_Reserve_System\t|spotlight:\tFederal_Reserve_System |\n",
      "|mention: Fed\t|gold:\tFederal_Reserve_System\t|aida:\tFederal_Reserve_System\t|spotlight:\tFederal_Reserve_System |\n",
      "http://aksw.org/N3/Reuters-128/104#char=0,130\n",
      "\n",
      "|mention: Yankee Cos Inc\t|gold:\tNIL\t|aida:\tNIL\t|spotlight:\tNIL |\n",
      "|mention: Eskey Inc\t|gold:\tEsky\t|aida:\tNIL\t|spotlight:\tNIL |\n",
      "http://aksw.org/N3/Reuters-128/14#char=0,1163\n",
      "\n",
      "|mention: Volkswagen AG\t|gold:\tVolkswagen_Group\t|aida:\tVolkswagen\t|spotlight:\tVolkswagen_Group |\n",
      "|mention: VW\t|gold:\tVolkswagen_Group\t|aida:\tNIL\t|spotlight:\tVolkswagen |\n",
      "|mention: VW\t|gold:\tVolkswagen_Group\t|aida:\tNIL\t|spotlight:\tVolkswagen |\n",
      "|mention: VW\t|gold:\tVolkswagen_Group\t|aida:\tNIL\t|spotlight:\tVolkswagen |\n",
      "|mention: VW\t|gold:\tVolkswagen_Group\t|aida:\tNIL\t|spotlight:\tVolkswagen |\n",
      "|mention: VW\t|gold:\tVolkswagen_Group\t|aida:\tNIL\t|spotlight:\tVolkswagen |\n",
      "http://aksw.org/N3/Reuters-128/105#char=0,1870\n",
      "\n",
      "|mention: Chicago Mercantile Exchange\t|gold:\tCME_Group\t|aida:\tChicago_Mercantile_Exchange\t|spotlight:\tChicago_Mercantile_Exchange |\n",
      "|mention: Paul OKelly\t|gold:\tNIL\t|aida:\tNIL\t|spotlight:\tNIL |\n",
      "|mention: Securities and Exchange Commission\t|gold:\tU.S._Securities_and_Exchange_Commission\t|aida:\tSecurities_Commission_(Brazil)\t|spotlight:\tU.S._Securities_and_Exchange_Commission |\n",
      "|mention: CME\t|gold:\tCME_Group\t|aida:\tChicago_Mercantile_Exchange\t|spotlight:\tChicago_Mercantile_Exchange |\n",
      "|mention: CME\t|gold:\tCME_Group\t|aida:\tChicago_Mercantile_Exchange\t|spotlight:\tChicago_Mercantile_Exchange |\n",
      "|mention: CME\t|gold:\tCME_Group\t|aida:\tChicago_Mercantile_Exchange\t|spotlight:\tChicago_Mercantile_Exchange |\n",
      "|mention: CME\t|gold:\tCME_Group\t|aida:\tChicago_Mercantile_Exchange\t|spotlight:\tChicago_Mercantile_Exchange |\n",
      "|mention: CME\t|gold:\tCME_Group\t|aida:\tChicago_Mercantile_Exchange\t|spotlight:\tChicago_Mercantile_Exchange |\n",
      "|mention: John Sandner\t|gold:\tJohn_F._Sandner\t|aida:\tJohn_F._Sandner\t|spotlight:\tNIL |\n",
      "|mention: Leo Melamed\t|gold:\tLeo_Melamed\t|aida:\tLeo_Melamed\t|spotlight:\tLeo_Melamed |\n",
      "|mention: Melamed\t|gold:\tLeo_Melamed\t|aida:\tLeo_Melamed\t|spotlight:\tMelamed |\n",
      "http://aksw.org/N3/Reuters-128/34#char=0,912\n",
      "\n",
      "http://aksw.org/N3/Reuters-128/70#char=0,282\n",
      "\n",
      "http://aksw.org/N3/Reuters-128/104#char=0,130\n",
      "\n",
      "http://aksw.org/N3/Reuters-128/14#char=0,1163\n",
      "\n",
      "http://aksw.org/N3/Reuters-128/105#char=0,1870\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for article in processed_both:\n",
    "    doc_id = article.identifier\n",
    "    print(doc_id)\n",
    "    print()\n",
    "    for mention in article.entity_mentions:\n",
    "        print('|mention: %s\\t|gold:\\t%s\\t|aida:\\t%s\\t|spotlight:\\t%s |' % (mention.mention, mention.gold_link, mention.aida_link, mention.spotlight_link))\n",
    "        \n",
    "system_decisions = [] \n",
    "gold_decisions = []\n",
    "for article in processed_both:\n",
    "    doc_id = article.identifier\n",
    "    print(doc_id)\n",
    "    print()\n",
    "    #print(article.entity_mentions.mention)\n",
    "    #print('|mention: %s\\t|gold:\\t%s\\t|aida:\\t%s\\t|spotlight:\\t%s |' % (mention.mention, mention.gold_link, mention.aida_link, mention.spotlight_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1b** Write code that evaluates the two systems on this dataset by computing their overall precision, recall, and F1-score. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to compute the precision, recall, and F1-score for each of the systems on this dataset\n",
    "def evaluate_entity_linking(system_decisions, gold_decisions):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1-score by comparing two paired lists of: system decisions and gold data decisions.\n",
    "    \"\"\"\n",
    "    tp=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "    \n",
    "    for gold_entity,system_entity in zip(gold_decisions,system_decisions):\n",
    "        if gold_entity=='NIL' and system_entity=='NIL': continue\n",
    "        if gold_entity==system_entity:\n",
    "            tp+=1\n",
    "        else:\n",
    "            if gold_entity!='NIL':\n",
    "                fn+=1\n",
    "            if system_entity!='NIL':\n",
    "                fp+=1\n",
    "\n",
    "    print('TP: %d; \\nFP: %d, \\nFN: %d' % (tp, fp, fn))            \n",
    "\n",
    "    precision=tp/(tp+fp)\n",
    "    recall=tp/(tp+fn)\n",
    "    f1=2*precision*recall/(precision+recall)\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1c** What is the F1-score per system? Which system performs better? Is that also the better system in terms of precision and recall? Which is higher and what does that mean (hint: think of NIL entities)?(5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excersize 4 (NEL): Qualitative analysis [Points: 15] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2a** Check the entity disambiguation by AIDA against the gold entities on the document with identifier \"http://aksw.org/N3/Reuters-128/82#char=0,1370\" (write code to print the entity mentions, gold links and AIDA links). (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in this document that one of the mentions of \"Tokyo\" is disambiguated wrongly by AIDA as `Tokyo` (it should be `Tokyo_Stock_Exchange`). Knowing how AIDA works, what would be your explanation for this error? (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2b** Check the entity disambiguation by Spotlight against the gold entities on the document \"http://aksw.org/N3/Reuters-128/36#char=0,1146\" (write code to print the entity mentions, gold links and Spotlight links). (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in this document that the mention of \"Group of Seven\" is disambiguated wrongly by Spotlight as `G8` (it should be `G7`). Knowing how Spotlight works, what would be your explanation for this error? (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2c** In the document with identifier \"http://aksw.org/N3/Reuters-128/67#char=0,1627\":\n",
    "- both systems correctly decide that \"Michel Dufour\" is a `NIL` entity with no representation in the English Wikipedia. \n",
    "- however, Spotlight later decides that \"Dufour\" refers to `Guillaume-Henri_Dufour`\n",
    "\n",
    "How would you help Spotlight fix this error? (Hint: think of how you would know that \"Dufour\" is a NIL entity in that document) (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
