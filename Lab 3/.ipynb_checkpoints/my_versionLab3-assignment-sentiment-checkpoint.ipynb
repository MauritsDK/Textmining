{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the LAB-2 assignment of the Text Mining course. It is about sentiment analysis.\n",
    "\n",
    "The aims of the assignment are:\n",
    "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
    "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
    "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
    "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
    "* Learn how to evaluate the results qualitatively (by examining the data) \n",
    "* Get insight into differences between the two applied methods\n",
    "* Get insight into the effects of using linguistic preprocessing\n",
    "* Be able to describe differences between the two methods in terms of their results\n",
    "* Get insight into issues when applying these methods across different  domains\n",
    "\n",
    "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
    "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
    "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: VADER assignments\n",
    "\n",
    "\n",
    "### Preparation (nothing to submit):\n",
    "To be able to answer the VADER questions you need to know how the tool works. \n",
    "* Read more about the VADER tool in [this blog](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html).  \n",
    "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
    "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
    "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
    "\n",
    "\n",
    "### [3.5 points] Question1:\n",
    "\n",
    "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question1 answers:\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369} \n",
    "```\n",
    "sentence 1 is 0% negative, 19.2% neutral and 80.8% positive, the whole sentence is labeled as positive. This is very reasonable because love has a sentiment rating of 3.2.\n",
    "\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "```\n",
    "62.7% negative, 37.3% neutral and 0% positive, overall negative.\n",
    "\"don't\" changes the overall rating from positive to negative.\n",
    "\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "```\n",
    "13.3%neutral and 86.7% positive. The sentence is more positive than than sentence 1, this is because the emoticon also has an positive sentiment rating.\n",
    "\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "```\n",
    "50.8% neutral and 49.2% negative. This sentence has the highest score for neutral, however it is almost 50/50 with negative. Which is understandable as the sentence is a statement about a 'negative' thing. \n",
    "'Ruins' is categorized as a negative word with a sentiment rating of -1.9, skewing vader's output towards negative. Hence the compound score of -0.4404. \n",
    "\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "```\n",
    "51% neutral and 49% positive. With the addition of 'certainly not' the sentiment has shifted from 50/50 neutral and negative to 50/50 neutral and positive. As 'certainly not' negates the negative sentiment of the word 'ruins'. Changing the overall sentiment of the sentence form negative to positive .   \n",
    "\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "```\n",
    "28.6% negative and 71.4% neutral. However the compound score is very negative hence the sentence will be labelled a being negative whilst it actually is neutral. This is because the word 'lies' can be interpreted as 'lying' implying something negative, it has a sentiment score of -1.8 skewing vaders decision towards a negative sentiment.\n",
    "\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```\n",
    "66.7% neutral and 33.3% positive. This neutral sentence is labelled as positive since the only word carrying sentiment is 'like', which has a positive sentiment. However the context in which 'like' is used makes it neutral, but vader of course does not take that into account.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. \n",
    "\n",
    "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
    "For each tweet, you should insert:\n",
    "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
    "* the text of the tweet\n",
    "* the Tweet-URL\n",
    "\n",
    "from:\n",
    "```\n",
    "    \"1\": {\n",
    "        \"sentiment_label\": \"\",\n",
    "        \"text_of_tweet\": \"\",\n",
    "        \"tweet_url\": \"\",\n",
    "```\n",
    "to:\n",
    "```\n",
    "\"1\": {\n",
    "        \"sentiment_label\": \"positive\",\n",
    "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
    "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your tweets with human annotation in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sklearn\n",
    "import pathlib\n",
    "import spacy\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_files\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import classification_report\n",
    "nlp = spacy.load('en') # en_core_web_sm\n",
    "vader_model = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweets = json.load(open('my_tweets.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'sentiment_label': 'positive', 'text_of_tweet': 'Happy Birthday Hoseok!  I Hope you spend your birthday with everyone you love ', 'tweet_url': ''}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in my_tweets.items():\n",
    "    print(id_, tweet_info)\n",
    "    break"
   ]
  },
  {
   "attachments": {
    "Screenshot%20from%202020-02-29%2019-45-21.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "\n",
    "![Screenshot%20from%202020-02-29%2019-45-21.png](attachment:Screenshot%20from%202020-02-29%2019-45-21.png)\n",
    "\n",
    "In the above table you can see the output of the quantitave elavualion. here is given the Precision, recall ,and the f1 score. The precision is the ratio of the true positives divided by the true positives and the false positives. so for example 53.6 precent of the time when vader labels a tweet as positive the tweets was correctly labeled.\n",
    "The Recall is the ratio of true positives divided by the true positives and false negative. So again for positive tweets 78.9 precent of the positives tweets were labeled as positived by Vader. Both scores are important to do calculate the accuraccy of Vader alone the scores don't say much you could for instance have a model with perfect recall for positive tweets simply by labeling everything as positive. \n",
    "\n",
    "The F score is defined as the weighted harmonic mean of the test's precision and recall. This is for a combitation of recall and precision and is the most important metric for are examination. Based on this Vader best able to predict positive tweets followed by negative and lastly neutral tweets.\n",
    "\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those.\n",
    "\n",
    "### We selected the following not correctly classified tweets as positive.\n",
    "\n",
    " \n",
    " He is happpy:). Was classified as neutral. Don't quite understand why this was classified neutral okay happy is directed at he but there is as smile emoji that ends the sentence does vader only assign as possitive it is directed at the self \n",
    " \n",
    " Incredible! Was classified as neutral. which i now see was missclassified by me.\n",
    " \n",
    " Hillaty Clinton is my fovorite person in the world. Was classified as neutral. Favorite was miss spelled which could be the reason for this.  \n",
    " \n",
    " its the only way i can enjoy fifa, love ur vids.  was classified as neutral. bad spelling maybe \n",
    " \n",
    " Everything is possible If your thought is positive.  Was classified as neutral it probally is closer to neutral then possitive\n",
    " \n",
    "### We selected the following not correctly classified tweets as neutral.\n",
    "\n",
    "Vatican confirms Pope Francis and two aides test positive for Coronavirus - MCM. this was classified as positive which is a shame test positive and the word virus could have pushed the shale to negative \n",
    " \n",
    "Gene Sarazen was the first golfer of the modern era to complete the career Grand Slam Trophy Read more about the 1932 Open Champion here. Was classified as positive\n",
    "\n",
    "but i'll probably be awake until 3 am for no reason. Was classified as negative which in heignshight is the correct classifacation \n",
    "\n",
    "To understand a paper better, I will write down some notes and will share them here. was clasified as positive\n",
    "\n",
    "### We selected the following not correctly classified tweets as negative.\n",
    "\n",
    "i’m so sad hope he gets well soon omg. Has been classified as positive. This was also in correctly classivief the sentiment is positive the emotion is negative\n",
    "\n",
    "If Trump wasn’t so busy firing his scientists and calling the coronavirus a hoax, we might be better prepared for it. Has been classified as positive. Was classified as positieve this clearly an attack on thrump\n",
    "\n",
    "Some real talk from @drdrew about how the media is reckless in its coronavirus coverage- Has been classified as positive.\n",
    "\n",
    "I’m sad to learn the founder of #TraderJoes has died, but I’m very excited to meet the cauliflower version of him coming soon. Has been classified as positive.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def run_vader(textual_unit, \n",
    "              lemmatize=False,\n",
    "              parts_of_speech_to_consider=set(),\n",
    "              verbose=1):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "    \n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -empty set -> all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "    \n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "        \n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-': \n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print('INPUT SENTENCE', sent)\n",
    "        print('INPUT TO VADER', input_to_vader)\n",
    "        print('VADER OUTPUT', scores)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0.0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INPUT SENTENCE I Hope you spend your birthday with everyone you love\n",
      "INPUT TO VADER ['happy', 'birthday', 'Hoseok', '!', ' ', 'I', 'hope', 'you', 'spend', 'your', 'birthday', 'with', 'everyone', 'you', 'love']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.448, 'pos': 0.552, 'compound': 0.902}\n",
      "\n",
      "INPUT SENTENCE I am so in love with her I can’t even explain it\n",
      "INPUT TO VADER ['I', 'be', 'so', 'in', 'love', 'with', 'her', 'I', 'can', 'not', 'even', 'explain', 'it']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.691, 'pos': 0.309, 'compound': 0.6682}\n",
      "\n",
      "INPUT SENTENCE He is happpy:)\n",
      "INPUT TO VADER ['He', 'be', 'happpy', ':', ')']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "INPUT SENTENCE He has more Yaya requests coming up so expect more panda goodness\n",
      "INPUT TO VADER ['I', 'think', 'my', 'friend', 'rele', 'like', 'Yaya', ',', 'another', 'commission', 'for', 'the', 'same', 'friend', '<3', 'He', 'have', 'more', 'Yaya', 'request', 'come', 'up', 'so', 'expect', 'more', 'panda', 'goodness']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.57, 'pos': 0.43, 'compound': 0.9334}\n",
      "\n",
      "INPUT SENTENCE Part.1 Pleading faceRed heart y’all so beautiful beautiful BEAUTIFULLLLLLLLLLLLLLLL\n",
      "INPUT TO VADER ['Part.1', 'plead', 'faceRed', 'heart', 'y’', 'all', 'so', 'beautiful', 'beautiful', 'BEAUTIFULLLLLLLLLLLLLLLL']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.445, 'pos': 0.555, 'compound': 0.8997}\n",
      "\n",
      "INPUT SENTENCE you beautiful lavender man.<3\n",
      "INPUT TO VADER ['happy', 'birthday', ',', 'you', 'beautiful', 'lavender', 'man.<3']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.345, 'pos': 0.655, 'compound': 0.8225}\n",
      "\n",
      "INPUT SENTENCE Incredible!\n",
      "INPUT TO VADER ['incredible', '!']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "INPUT SENTENCE I love you so much\n",
      "INPUT TO VADER ['I', 'love', 'you', 'so', 'much']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.417, 'pos': 0.583, 'compound': 0.6369}\n",
      "\n",
      "INPUT SENTENCE Hillaty Clinton is my fovorite person in the world\n",
      "INPUT TO VADER ['Hillaty', 'Clinton', 'be', 'my', 'fovorite', 'person', 'in', 'the', 'world']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "INPUT SENTENCE Happy birthday\n",
      "INPUT TO VADER ['happy', 'birthday']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.213, 'pos': 0.787, 'compound': 0.5719}\n",
      "\n",
      "INPUT SENTENCE Smile and keep moving\n",
      "INPUT TO VADER ['happy', '#', 'LeapDay', '#', 'cancerfighter', ' ', 'smile', 'and', 'keep', 'move']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.446, 'pos': 0.554, 'compound': 0.7351}\n",
      "\n",
      "INPUT SENTENCE good morning baes <3\n",
      "INPUT TO VADER ['good', 'morning', 'bae', '<3']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.256, 'pos': 0.744, 'compound': 0.7003}\n",
      "\n",
      "INPUT SENTENCE its the only way i can enjoy fifa, love ur vids\n",
      "INPUT TO VADER ['#', 'itshaber', 'i', 'do', 'not', 'know', 'who', 'to', 'buy', ',', 'i', 'know', 'its', 'very', 'attacking', 'but', 'its', 'the', 'only', 'way', 'i', 'can', 'enjoy', 'fifa', ',', 'love', 'ur', 'vid']\n",
      "VADER OUTPUT {'neg': 0.069, 'neu': 0.608, 'pos': 0.323, 'compound': 0.8736}\n",
      "\n",
      "INPUT SENTENCE neutral\n",
      "INPUT TO VADER ['neutral']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "INPUT SENTENCE Let's keep our Positive Energy ^^\n",
      "INPUT TO VADER ['let', \"'s\", 'keep', 'our', 'Positive', 'Energy', '^^']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.467, 'pos': 0.533, 'compound': 0.6908}\n",
      "\n",
      "INPUT SENTENCE Who says twitter can't be used to foster a positive community?\n",
      "INPUT TO VADER ['feel', 'much', 'gratitude', 'for', 'my', 'community', 'of', 'fellow', 'nerd', ',', 'truth', 'seeker', ',', 'and', 'creative', 'humanist', 'here', 'on', 'twitter', '.', 'this', 'platform', 'be', 'enrich', 'for', 'me', 'and', 'I', 'appreciate', 'the', 'folk', 'I', 'regularly', 'engage', 'with', 'on', 'here', '.', 'who', 'say', 'twitter', 'can', 'not', 'be', 'use', 'to', 'foster', 'a', 'positive', 'community', '?']\n",
      "VADER OUTPUT {'neg': 0.04, 'neu': 0.65, 'pos': 0.31, 'compound': 0.9325}\n",
      "\n",
      "INPUT SENTENCE Everything is possible If your thought is positive.\n",
      "INPUT TO VADER ['everything', 'be', 'possible', 'if', 'your', 'thought', 'be', 'positive', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.66, 'pos': 0.34, 'compound': 0.5574}\n",
      "\n",
      "INPUT SENTENCE Vatican confirms Pope Francis and two aides test positive for Coronavirus - MCM\n",
      "INPUT TO VADER ['Vatican', 'confirm', 'Pope', 'Francis', 'and', 'two', 'aide', 'test', 'positive', 'for', 'Coronavirus', '-', 'MCM']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.753, 'pos': 0.247, 'compound': 0.5574}\n",
      "\n",
      "INPUT SENTENCE Think Young, Play Hard.\n",
      "INPUT TO VADER ['We', 'share', 'advice', 'on', 'how', 'to', 'play', ',', 'what', 'to', 'play', 'and', 'where', 'to', 'play', '.', 'think', 'Young', ',', 'play', 'Hard', '.']\n",
      "VADER OUTPUT {'neg': 0.056, 'neu': 0.476, 'pos': 0.468, 'compound': 0.8555}\n",
      "\n",
      "INPUT SENTENCE The 149th Open at Royal St George's 12-19 July 2020 #TheOpen\n",
      "INPUT TO VADER ['the', '149th', 'Open', 'at', 'Royal', 'St', 'George', \"'s\", '12', '-', '19', 'July', '2020', '#', 'TheOpen']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "INPUT SENTENCE Let us know using the poll below\n",
      "INPUT TO VADER ['what', 'score', 'would', 'you', 'get', 'on', 'the', '9th', 'hole', '?', 'think', 'face', 'let', 'us', 'know', 'use', 'the', 'poll', 'below']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "INPUT SENTENCE Vote now:\n",
      "INPUT TO VADER ['vote', 'now', ':']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "INPUT SENTENCE Gene Sarazen was the first golfer of the modern era to complete the career Grand Slam Trophy Read more about the 1932 Open Champion here\n",
      "INPUT TO VADER ['do', 'you', 'know', '?', 'Gene', 'Sarazen', 'be', 'the', 'first', 'golfer', 'of', 'the', 'modern', 'era', 'to', 'complete', 'the', 'career', 'Grand', 'Slam', 'Trophy', 'Read', 'more', 'about', 'the', '1932', 'Open', 'Champion', 'here']\n",
      "VADER OUTPUT {'neg': 0.075, 'neu': 0.725, 'pos': 0.2, 'compound': 0.6486}\n",
      "\n",
      "INPUT SENTENCE ,\n",
      "INPUT TO VADER ['the', 'merit', 'of', 'Our', 'Cocaine', '.', 'Advert', 'from', '1907', '.', ',']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.714, 'pos': 0.286, 'compound': 0.4215}\n",
      "\n",
      "INPUT SENTENCE Coca-Cola originally contained cocaine.\n",
      "INPUT TO VADER ['Coca', '-', 'Cola', 'originally', 'contain', 'cocaine', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "INPUT SENTENCE but i'll probably be awake until 3 am for no reason\n",
      "INPUT TO VADER ['i', 'be', 'so', 'tired', 'but', 'i', 'will', 'probably', 'be', 'awake', 'until', '3', 'be', 'for', 'no', 'reason']\n",
      "VADER OUTPUT {'neg': 0.308, 'neu': 0.692, 'pos': 0.0, 'compound': -0.5989}\n",
      "\n",
      "INPUT SENTENCE Neural magazine, new media art, electronic music, hacktivism, since 1993\n",
      "INPUT TO VADER ['neural', 'magazine', ',', 'new', 'medium', 'art', ',', 'electronic', 'music', ',', 'hacktivism', ',', 'since', '1993']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "INPUT SENTENCE Real-time detection and tracking of basketball players using deep neural networks\n",
      "INPUT TO VADER ['real', '-', 'time', 'detection', 'and', 'tracking', 'of', 'basketball', 'player', 'use', 'deep', 'neural', 'network']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "INPUT SENTENCE Scientists at Facebook AI have done what was previously considered out of reach for deep learning models, solving complex, symbolic math equations using a neural networks\n",
      "INPUT TO VADER ['scientist', 'at', 'Facebook', 'AI', 'have', 'do', 'what', 'be', 'previously', 'consider', 'out', 'of', 'reach', 'for', 'deep', 'learning', 'model', ',', 'solve', 'complex', ',', 'symbolic', 'math', 'equation', 'use', 'a', 'neural', 'network']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.888, 'pos': 0.112, 'compound': 0.2263}\n",
      "\n",
      "INPUT SENTENCE where did I error?\n",
      "INPUT TO VADER ['training', 'for', 'Neural', 'Knockdown', '.', 'I', 'download', 'my', 'tough', 'guy', 'voice', 'to', 'learn', 'Kefla', '.', 'where', 'do', 'I', 'error', '?']\n",
      "VADER OUTPUT {'neg': 0.244, 'neu': 0.756, 'pos': 0.0, 'compound': -0.4939}\n",
      "\n",
      "INPUT SENTENCE To understand a paper better, I will write down some notes and will share them here.\n",
      "INPUT TO VADER ['to', 'understand', 'a', 'paper', 'better', ',', 'I', 'will', 'write', 'down', 'some', 'note', 'and', 'will', 'share', 'them', 'here', '.']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.702, 'pos': 0.298, 'compound': 0.6249}\n",
      "\n",
      "INPUT SENTENCE This is the future.(blame@janet_laible  for the training data)\n",
      "INPUT TO VADER ['scented', 'candle', 'from', 'a', 'neural', 'net', 'train', 'on', '1000', 'exist', 'candle', 'the', 'neural', 'net', 'be', 'base', 'on', 'math', ',', 'so', 'it', 'be', 'infallible', '.', 'this', 'be', 'the', 'future.(blame@janet_laible', ' ', 'for', 'the', 'training', 'datum', ')']\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "INPUT SENTENCE the roads in california are so shitty because everyone gets distracted paving them because they came up with a good idea (restaurant that has a recording studio in it) while using cocaine\n",
      "INPUT TO VADER ['the', 'road', 'in', 'california', 'be', 'so', 'shitty', 'because', 'everyone', 'get', 'distract', 'pave', 'them', 'because', 'they', 'come', 'up', 'with', 'a', 'good', 'idea', '(', 'restaurant', 'that', 'have', 'a', 'recording', 'studio', 'in', 'it', ')', 'while', 'use', 'cocaine']\n",
      "VADER OUTPUT {'neg': 0.186, 'neu': 0.735, 'pos': 0.079, 'compound': -0.6015}\n",
      "\n",
      "INPUT SENTENCE whenever i’m sad,, just to seek comfort,, i’m gonna go cry now\n",
      "INPUT TO VADER ['constantly', 'find', 'myself', 'go', 'back', 'to', 'my', 'letter', '&', 'palancas', 'whenever', 'i', 'be', 'sad', ',', ',', 'just', 'to', 'seek', 'comfort', ',', ',', 'i', 'be', 'go', 'to', 'go', 'cry', 'now']\n",
      "VADER OUTPUT {'neg': 0.224, 'neu': 0.686, 'pos': 0.09, 'compound': -0.5719}\n",
      "\n",
      "INPUT SENTENCE now i disappear for a lil bit and ponder my sad existence\n",
      "INPUT TO VADER ['now', 'i', 'disappear', 'for', 'a', 'lil', 'bit', 'and', 'ponder', 'my', 'sad', 'existence']\n",
      "VADER OUTPUT {'neg': 0.385, 'neu': 0.615, 'pos': 0.0, 'compound': -0.6124}\n",
      "\n",
      "INPUT SENTENCE wack ass\n",
      "INPUT TO VADER ['I', 'do', 'not', 'get', 'jealous', 'or', 'sad', 'i', 'get', 'uninterested', ',', 'they', 'can', 'have', 'ur', 'wack', 'ass']\n",
      "VADER OUTPUT {'neg': 0.329, 'neu': 0.548, 'pos': 0.124, 'compound': -0.6273}\n",
      "\n",
      "INPUT SENTENCE every time i see those compilations of my favorite mutuals i get so sad since i haven't made it onto a single one.\n",
      "INPUT TO VADER ['every', 'time', 'i', 'see', 'those', 'compilation', 'of', 'my', 'favorite', 'mutual', 'i', 'get', 'so', 'sad', 'since', 'i', 'have', 'not', 'make', 'it', 'onto', 'a', 'single', 'one', '.']\n",
      "VADER OUTPUT {'neg': 0.16, 'neu': 0.72, 'pos': 0.12, 'compound': -0.2479}\n",
      "\n",
      "INPUT SENTENCE soon omg\n",
      "INPUT TO VADER ['i', 'be', 'so', 'sad', 'hope', 'he', 'get', 'well', 'soon', 'omg']\n",
      "VADER OUTPUT {'neg': 0.231, 'neu': 0.409, 'pos': 0.36, 'compound': 0.2228}\n",
      "\n",
      "INPUT SENTENCE #\n",
      "INPUT TO VADER ['People', 'call', 'it', 'a', 'sad', 'day', 'for', 'german', 'football', 'because', 'people', 'be', 'insult', 'Hopp', '.', 'no', ',', 'it', '’', 'a', 'sad', 'day', 'for', 'german', 'football', 'because', 'it', 'show', 'the', 'DFB', 'care', 'far', 'more', 'about', 'the', 'fragile', 'ego', 'of', 'an', 'old', 'rich', 'white', 'man', 'than', 'they', 'do', 'about', 'racism', '.', '#']\n",
      "VADER OUTPUT {'neg': 0.27, 'neu': 0.614, 'pos': 0.116, 'compound': -0.8402}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INPUT SENTENCE i'm sad now\n",
      "INPUT TO VADER ['when', 'fluke', 'thank', \"'\", \"'\", 'pharm', \"'\", \"'\", 'I', 'feel', 'like', 'cry', ',', 'it', 'really', 'hit', 'me', 'hard', 'that', 'uwma', 'be', 'really', 'end', 'tomorrow', 'i', 'be', 'sad', 'now']\n",
      "VADER OUTPUT {'neg': 0.272, 'neu': 0.554, 'pos': 0.173, 'compound': -0.4336}\n",
      "\n",
      "INPUT SENTENCE It’s kind of sad.\n",
      "INPUT TO VADER ['He', '’', 'so', 'proud', 'of', 'be', 'a', 'dick', '.', 'It', '’', 'kind', 'of', 'sad', '.']\n",
      "VADER OUTPUT {'neg': 0.392, 'neu': 0.41, 'pos': 0.199, 'compound': -0.5106}\n",
      "\n",
      "INPUT SENTENCE If Trump wasn’t so busy firing his scientists and calling the coronavirus a hoax, we might be better prepared for it.\n",
      "INPUT TO VADER ['if', 'Trump', 'be', 'not', 'so', 'busy', 'fire', 'his', 'scientist', 'and', 'call', 'the', 'coronavirus', 'a', 'hoax', ',', 'we', 'may', 'be', 'better', 'prepared', 'for', 'it', '.']\n",
      "VADER OUTPUT {'neg': 0.08, 'neu': 0.65, 'pos': 0.269, 'compound': 0.6049}\n",
      "\n",
      "INPUT SENTENCE Some real talk from @drdrew about how the media is reckless in its coronavirus coverage-\n",
      "INPUT TO VADER ['some', 'real', 'talk', 'from', '@drdrew', 'about', 'how', 'the', 'media', 'be', 'reckless', 'in', 'its', 'coronavirus', 'coverage-']\n",
      "VADER OUTPUT {'neg': 0.162, 'neu': 0.838, 'pos': 0.0, 'compound': -0.4019}\n",
      "\n",
      "INPUT SENTENCE so far- is completely unjustified and insane.\n",
      "INPUT TO VADER ['freak', 'out', 'about', 'Trump', '’s', 'handling', 'of', 'this-', 'with', 'ZERO', 'death', 'in', 'the', 'US', 'so', 'far-', 'be', 'completely', 'unjustified', 'and', 'insane', '.']\n",
      "VADER OUTPUT {'neg': 0.352, 'neu': 0.648, 'pos': 0.0, 'compound': -0.8678}\n",
      "\n",
      "INPUT SENTENCE How sad\n",
      "INPUT TO VADER ['I', 'be', 'very', 'convinced', 'the', 'fgc', 'do', 'not', 'want', 'to', 'see', 'zoner', 'be', 'good', '.', 'how', 'sad']\n",
      "VADER OUTPUT {'neg': 0.204, 'neu': 0.519, 'pos': 0.278, 'compound': 0.3759}\n",
      "\n",
      "INPUT SENTENCE I’m sad to learn the founder of #TraderJoes has died, but I’m very excited to meet the cauliflower version of him coming soon.\n",
      "INPUT TO VADER ['I', 'be', 'sad', 'to', 'learn', 'the', 'founder', 'of', '#', 'TraderJoes', 'have', 'die', ',', 'but', 'I', 'be', 'very', 'excited', 'to', 'meet', 'the', 'cauliflow', 'version', 'of', 'him', 'come', 'soon', '.']\n",
      "VADER OUTPUT {'neg': 0.16, 'neu': 0.713, 'pos': 0.126, 'compound': 0.0102}\n",
      "\n",
      "INPUT SENTENCE Feeling very sad now Loudly crying faceLoudly crying faceKiss mark\n",
      "INPUT TO VADER ['RT', '.', 'feel', 'very', 'sad', 'now', 'loudly', 'cry', 'faceloudly', 'cry', 'facekiss', 'mark']\n",
      "VADER OUTPUT {'neg': 0.545, 'neu': 0.455, 'pos': 0.0, 'compound': -0.8622}\n",
      "\n",
      "INPUT SENTENCE My characters and my art, no take I cry.\n",
      "INPUT TO VADER ['this', 'little', 'shit', 'and', 'their', 'very', 'trustworthy', 'shadow', 'friend', 'be', 'now', 'relatively', 'design', '.', 'My', 'character', 'and', 'my', 'art', ',', 'no', 'take', 'I', 'cry', '.']\n",
      "VADER OUTPUT {'neg': 0.269, 'neu': 0.501, 'pos': 0.23, 'compound': -0.0645}\n",
      "\n",
      "INPUT SENTENCE me joking about my depression to mask my depression\n",
      "INPUT TO VADER ['me', 'joke', 'about', 'my', 'depression', 'to', 'mask', 'my', 'depression']\n",
      "VADER OUTPUT {'neg': 0.474, 'neu': 0.385, 'pos': 0.141, 'compound': -0.7351}\n",
      "\n",
      "INPUT SENTENCE So sad!!\n",
      "INPUT TO VADER ['this', 'be', 'not', 'the', 'way', 'to', 'go', '!', '!', 'so', 'sad', '!', '!']\n",
      "VADER OUTPUT {'neg': 0.392, 'neu': 0.608, 'pos': 0.0, 'compound': -0.7318}\n",
      "                                                                                                            precision    recall  f1-score   support\n",
      "\n",
      "Falling in love is easy, staying in love is a challenge, letting go is hard, and moving on is the hardest.      0.000     0.000     0.000         1\n",
      "                                                                                                  negative      0.875     0.778     0.824        18\n",
      "                                                                                                   neutral      0.636     0.467     0.538        15\n",
      "                                                                                                  positive      0.565     0.812     0.667        16\n",
      "\n",
      "                                                                                                  accuracy                          0.680        50\n",
      "                                                                                                 macro avg      0.519     0.514     0.507        50\n",
      "                                                                                              weighted avg      0.687     0.680     0.671        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = run_vader(the_tweet, lemmatize = to_lemmatize)# run vader\n",
    "    vader_label = vader_output_to_label(vader_output)# convert vader output to category\n",
    "    \n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "report = classification_report(gold,all_vader_output,digits = 3)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 4:\n",
    "Run VADER on the set of airline tweets with the following settings:\n",
    "\n",
    "* I - Run VADER (as it is) on the set of airline tweets \n",
    "* II -  Run VADER on the set of airline tweets after having lemmatized the text\n",
    "* III - Run VADER on the set of airline tweets with only adjectives\n",
    "* IV - Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "* V - Run VADER on the set of airline tweets with only nouns\n",
    "* VI - Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "* VII - Run VADER on the set of airline tweets with only verbs\n",
    "* VIII - Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "\n",
    "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
    "* [3 points] b. Compare the scores and explain what they tell you.\n",
    "* - Does lemmatisation help? Explain why or why not.\n",
    "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the airline tweet files:\n",
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "airline_tweets= load_files(str(airline_tweets_folder))\n",
    "\n",
    "# And making a function to (easily) run VADER on tweets using different settings:\n",
    "def run_vader_on_tweets(tweets,\n",
    "                        lemmatize_value=False,\n",
    "                        pos_value=set()):\n",
    "    vader_label =[]\n",
    "    gold = []\n",
    "\n",
    "    for tweet, label_int in zip(tweets.data,tweets.target):\n",
    "        tweet_ = tweet.decode(\"utf-8\")\n",
    "        vader_output = run_vader(tweet_, lemmatize=lemmatize_value,parts_of_speech_to_consider= pos_value)\n",
    "        vader_output_label = vader_output_to_label(vader_output)\n",
    "        vader_label.append(vader_output_label)\n",
    "        gold_label = airline_tweets.target_names[label_int]\n",
    "        gold.append(gold_label)\n",
    "    report = classification_report(gold, vader_label, digits =3)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4a.** Generate for all separate experiments the classification report, i.e., Precision, Recall, and F1 scores per category as well as micro and macro averages. Use a different code cell (or multiple code cells) for each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.797     0.515     0.625      1750\n",
      "     neutral      0.605     0.506     0.551      1515\n",
      "    positive      0.559     0.884     0.685      1490\n",
      "\n",
      "    accuracy                          0.628      4755\n",
      "   macro avg      0.654     0.635     0.621      4755\n",
      "weighted avg      0.661     0.628     0.620      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#I Run VADER (as it is) on the set of airline tweets \n",
    "run_vader_on_tweets(airline_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.787     0.523     0.628      1750\n",
      "     neutral      0.599     0.490     0.539      1515\n",
      "    positive      0.556     0.879     0.682      1490\n",
      "\n",
      "    accuracy                          0.624      4755\n",
      "   macro avg      0.648     0.631     0.616      4755\n",
      "weighted avg      0.655     0.624     0.617      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#II - Run VADER on the set of airline tweets after having lemmatized the text\n",
    "run_vader_on_tweets(airline_tweets, lemmatize_value = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.863     0.213     0.342      1750\n",
      "     neutral      0.407     0.891     0.559      1515\n",
      "    positive      0.675     0.455     0.544      1490\n",
      "\n",
      "    accuracy                          0.505      4755\n",
      "   macro avg      0.648     0.520     0.481      4755\n",
      "weighted avg      0.659     0.505     0.474      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#III - Run VADER on the set of airline tweets with only adjectives\n",
    "run_vader_on_tweets(airline_tweets, pos_value = {'ADJ'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.861     0.213     0.342      1750\n",
      "     neutral      0.407     0.891     0.559      1515\n",
      "    positive      0.675     0.455     0.543      1490\n",
      "\n",
      "    accuracy                          0.505      4755\n",
      "   macro avg      0.648     0.520     0.481      4755\n",
      "weighted avg      0.658     0.505     0.474      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#IV - Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "run_vader_on_tweets(airline_tweets, lemmatize_value = True,pos_value = {'ADJ'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.721     0.137     0.230      1750\n",
      "     neutral      0.359     0.825     0.500      1515\n",
      "    positive      0.546     0.344     0.422      1490\n",
      "\n",
      "    accuracy                          0.421      4755\n",
      "   macro avg      0.542     0.435     0.384      4755\n",
      "weighted avg      0.551     0.421     0.376      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#V - Run VADER on the set of airline tweets with only nouns\n",
    "run_vader_on_tweets(airline_tweets, pos_value = {'NOUN'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.709     0.150     0.248      1750\n",
      "     neutral      0.359     0.816     0.498      1515\n",
      "    positive      0.534     0.336     0.413      1490\n",
      "\n",
      "    accuracy                          0.421      4755\n",
      "   macro avg      0.534     0.434     0.386      4755\n",
      "weighted avg      0.543     0.421     0.379      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#VI - Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "run_vader_on_tweets(airline_tweets, lemmatize_value = True,pos_value = {'NOUN'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.775     0.284     0.416      1750\n",
      "     neutral      0.383     0.809     0.520      1515\n",
      "    positive      0.568     0.349     0.432      1490\n",
      "\n",
      "    accuracy                          0.472      4755\n",
      "   macro avg      0.576     0.481     0.456      4755\n",
      "weighted avg      0.585     0.472     0.454      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#VII - Run VADER on the set of airline tweets with only verbs\n",
    "run_vader_on_tweets(airline_tweets, pos_value = {'VERB'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.742     0.292     0.419      1750\n",
      "     neutral      0.379     0.781     0.510      1515\n",
      "    positive      0.566     0.358     0.439      1490\n",
      "\n",
      "    accuracy                          0.469      4755\n",
      "   macro avg      0.562     0.477     0.456      4755\n",
      "weighted avg      0.571     0.469     0.454      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#VIII - Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "run_vader_on_tweets(airline_tweets,lemmatize_value = True, pos_value = {'VERB'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4b.** Compare the scores and explain what they tell you.\n",
    "+ Does lemmatisation help?  \n",
    "\n",
    "The classification reports show that lemmatisation, overall, does not make a huge difference. There is a slight decrease in precision, recall and with them the f1-score, micro and macro average when the text is lemmatized. However, the positive precision and recall when looking at just the verbs and the negative precision and recall when only looking at nouns have a slight increase when they are lemmatized. With lemmatisation the verb goes back to its lemma, with this some meaning can get changed/lost, this could explain the classification report results.\n",
    "\n",
    "+ Are all parts of speech equally important for sentiment analysis?\n",
    "\n",
    "The precision, recall and with it the f1-scores are not convincingly better or worse when comparing the specific parts of speech with the complete sentence analyses. However, the micro, macro and weighted averages are consistently lower when looking at the specific parts of speech. This can be explained by the loss of nuance when only looking at a specific part of speech. Words can have different meaning when used in a certain context, this context disappears with part of speech tagging and therefor the meaning can be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: scikit-learn assignments\n",
    "### [4 points] Question 5\n",
    "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
    "\n",
    "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
    "+ Train with different settings:\n",
    "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
    "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sklearn\n",
    "import numpy\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the airport tweets\n",
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_path = cwd.joinpath('airlinetweets')\n",
    "airline_tweets = load_files(str(airline_tweets_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_and_train_tweets(df, vectorizer):\n",
    "    #vectorize all data \n",
    "    airline_vec = CountVectorizer(min_df= df, tokenizer=nltk.word_tokenize, stop_words=stopwords.words('english')) \n",
    "    airline_counts = airline_vec.fit_transform(airline_tweets.data)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "    \n",
    "    #check which vectorizer is used, splits the data, trains and tests the classifier on the data, \n",
    "    #the corresponding report is passed on as output of the function, if vectorizer is not recognized-> handled as well\n",
    "    if vectorizer == 'tfidf': \n",
    "        tweets_train, tweets_test, y_train, y_test = train_test_split(airline_tfidf, airline_tweets.target, test_size = 0.20)\n",
    "        tfidf_clf = MultinomialNB().fit(tweets_train, y_train)\n",
    "        tfidf_pred = tfidf_clf.predict(tweets_test)\n",
    "        report = sklearn.metrics.classification_report(y_true=y_test, y_pred = tfidf_pred, digits=3)\n",
    "        \n",
    "    elif vectorizer == 'count':\n",
    "        tweets_train, tweets_test, y_train, y_test = train_test_split(airline_counts, airline_tweets.target, test_size = 0.20)\n",
    "        count_clf = MultinomialNB().fit(tweets_train, y_train)\n",
    "        count_pred = count_clf.predict(tweets_test)\n",
    "        report = sklearn.metrics.classification_report(y_true=y_test, y_pred = count_pred, digits=2)\n",
    "    else: \n",
    "        report = ('%s vectorizer not defined' %vectorizer)\n",
    "        \n",
    "    return(report)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1 point] a. Generate a classification_report for all experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive bayes airline - default - TF-IDF representation, min_df=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorize_and_train_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fde620215098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorize_and_train_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorize_and_train_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "print(vectorize_and_train_tweets(2,'tfidf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive bayes airline - default - BoW representation, min_df=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marin\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87       355\n",
      "           1       0.86      0.70      0.78       315\n",
      "           2       0.81      0.88      0.84       281\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.83      0.83       951\n",
      "weighted avg       0.84      0.83      0.83       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(vectorize_and_train_tweets(2,'count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive bayes airline - default - TF-IDF representation, min_df=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marin\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.806     0.901     0.851       333\n",
      "           1      0.827     0.736     0.779       311\n",
      "           2      0.844     0.831     0.837       307\n",
      "\n",
      "    accuracy                          0.824       951\n",
      "   macro avg      0.826     0.823     0.822       951\n",
      "weighted avg      0.825     0.824     0.823       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(vectorize_and_train_tweets(5,'tfidf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(vectorize_and_train_tweets(10,'tfidf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marin\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.786     0.909     0.843       340\n",
      "           1      0.830     0.727     0.775       308\n",
      "           2      0.872     0.828     0.849       303\n",
      "\n",
      "    accuracy                          0.824       951\n",
      "   macro avg      0.829     0.821     0.823       951\n",
      "weighted avg      0.827     0.824     0.823       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(vectorize_and_train_tweets(10,'tfidf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
    "    + which category performs best, is this the case for any setting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all settings tested the highest overall scoring is category 0, the negative category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_tweets.target_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For; count-, tfidf vectorizer, min_df = 2, 5 and 10, all values for precision, recall and the f1-score tended to follow a certain pattern. This pattern would have category 0 as the highest scoring, on every metric except precision. Here it is slightly lower than the precision of the other categories. Furthermore it can be seen that whilst category 0 performs best overall, category 2, positive, is in a sure second position for every setting. Category 1, neutral, is the lowest scoring category everywhere, this could be because it is harder to recognize since the sentiment is based on presence of certain positive or negative words. \n",
    "\n",
    "The general shape the classification report follows for all settings\n",
    "```\n",
    "        precision   recall   f1-score  \n",
    "   0      0.821     0.869     0.844    <- highest overall scores  \n",
    "   1      0.779     0.719     0.748    <-lowest scores present   \n",
    "   2      0.834     0.840     0.837    <- second highest everywhere, except for precision where it scores highest  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " + does the frequency threshold affect the scores? Why or why not according to you?\n",
    " \n",
    "The accuracy, macro- and weighted averages go up slightly when the frequency threshold is increased. The overall precision, recall and f1 per category vary a little but not in a noticeble pattern. \n",
    "One possible explanation for the increased accuracy and averages when the frequency treshold is heightened, the remaining terms analyzed have a higher chance to actually contain sentiment, as they are commonly used across the documents. \n",
    "This trend of increasing the accuracy through heigtening the frequency threshold will probably not hold for a much higher frequency threshold, as too many important words could be filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 6: Inspecting the best scoring features \n",
    "\n",
    "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
    "* [3 points] b. Look at the lists and consider the following issues: \n",
    "    + [1 point] Which features did you expect for each separate class and why?\n",
    "    + [1 point] Which features did you not expect and why ? \n",
    "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_features_per_class(vectorizer,classifier,n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "# example of how to call from notebook:\n",
    "#important_features_per_class(airline_vec, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not  be graded)] Question 7\n",
    "Train the model on airline tweets and test it on your own set of tweets\n",
    "+ Train the model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "+ Apply the model on your own set of tweets and generate the classification report\n",
    "* [1 point] a. Carry out a quantitative analysis.\n",
    "* [1 point] b. Carry out an error analysis on 10 correctly and 10 incorrectly classified tweets and discuss them\n",
    "* [2 points] c. Compare the results (cf. classification report) with the results obtained by VADER on the same tweets and discuss the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
    "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings \n",
    "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
    "* [1 point] c. Discuss whether the model achieved what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
