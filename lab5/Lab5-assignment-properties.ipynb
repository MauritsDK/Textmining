{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab5 - Assignment 5 about extraction of properties\n",
    "\n",
    "Copyright, Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL\n",
    "\n",
    "This notebook describes the LAB-5 assignment of the Text Mining course. It is about Property Extraction.\n",
    "\n",
    "**Due**: 17 Mar at 23:59\n",
    "\n",
    "**How to submit**: Please submit your assignment using Canvas (see *Assignments* -> *Lab Session Property Extraction*). Convert your notebook to PDF (in JupyterLab, this can be done by clicking on *File* in the menu bar, select *Export Notebook As*, then select *Export Notebook to PDF*)\n",
    "\n",
    "**Points**: each exercise is suffixed with the number of points you can obtain for the exercise.\n",
    "\n",
    "**Assignment goals**:\n",
    "* Get insight into the challenges of entity property extraction.\n",
    "* Learn how to build a transparent property extraction method based on patterns.\n",
    "* Get insight into the pros and cons of two pattern-based property extraction methods.\n",
    "* Be able to run your extractors on unseen documents from Wikipedia.\n",
    "* Be able to evaluate property extractors.\n",
    "\n",
    "In this assignment, the main focus lies on creating your own pattern-based property extractors. You are then going to run them on Wikipedia texts, evaluate them against gold values, and reflect on their relative performance.\n",
    "\n",
    " We recommend that you go through the notebooks in the following order:\n",
    "* *Read the assignment (see below)*\n",
    "* *Lab5-Property-extraction.ipynb*\n",
    "* *Answer the questions of the assignment (see below) using the provided notebooks and submit*\n",
    "\n",
    "**Hint:** in the explanation notebook, we had an example about extraction of properties with substring matching and with dependencies. You can use much of that code here, but make sure you make the right adjustments.\n",
    "\n",
    "**Good luck & have fun!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import lab5_utils as utils\n",
    "\n",
    "model=\"en_core_web_sm\"\n",
    "\n",
    "nlp = spacy.load(model)\n",
    "# print(\"Info: Loaded model '%s'\" % model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extracting properties with substring matching (12 points)\n",
    "\n",
    "**Exercise 1a** Write code that extracts the birth year of a person by using substring matching. (4 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_birth_year_regex(doc, patterns):\n",
    "    # Extract the birth year of a person with regular expressions\n",
    "    property_value_type='DATE'\n",
    "    target_entity_type='PERSON'\n",
    "    \n",
    "    # the following 3 lines merge entities and noun chunks into one token\n",
    "    # this is useful in our cases, so we will always do it.\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "\n",
    "    relations = {}\n",
    "    \n",
    "    # step Ia - generate possible property values\n",
    "    dates=utils.get_entities_of_type(property_value_type, doc)\n",
    "    for date in dates:\n",
    "        # step Ib - is one of our patterns found before the date \n",
    "        if utils.pattern_found_on_the_left(doc, date.i, patterns):\n",
    "            # step II - find the closest entity of some target type\n",
    "            pers=utils.find_closest_entity(doc.ents, date.idx, target_entity_type)\n",
    "            # step III - normalize the year\n",
    "            year=utils.extract_year_from_date(date.text)\n",
    "            if year and pers:\n",
    "                relations[pers]=year\n",
    "\n",
    "    return relations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1b** Test your *birth year substring matching extractor* in the following way. \n",
    "\n",
    "* Write a sentence on which you expect that the extractor *WILL* work. \n",
    "* Write a sentence on which you expect that the extractor *WILL NOT* work. \n",
    "\n",
    "Run your extractor on both sentences and print the results. Make sure that the results are as expected. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sentence \u001b[1m WILL\u001b[0m work: Peter was born in 1975.\n",
      "{'Peter': 1975}\n",
      "\n",
      "This sentence \u001b[1m WILL NOT\u001b[0m work: Peter Pan's dog was born in 1990\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "born_patterns=['born in', 'birthdate', 'born on']\n",
    "\n",
    "text='Peter was born in 1975.'\n",
    "text2 = \"Peter Pan's dog was born in 1990\"\n",
    "\n",
    "print( \"This sentence \\033[1m WILL\\033[0m work:\",text)\n",
    "birth_year_relations=extract_birth_year_regex(nlp(text), born_patterns)\n",
    "print(birth_year_relations)\n",
    "print()\n",
    "print( \"This sentence \\033[1m WILL NOT\\033[0m work:\",text2)\n",
    "birth_year_relations2=extract_birth_year_regex(nlp(text2), born_patterns)\n",
    "print(birth_year_relations2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1c** Write code that extracts the manufacturer of a device by using substring matching. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_manufacturer_regex(doc, patterns, main_entity):\n",
    "    # Extract the manufacturer of a device by using regular expressions\n",
    "    property_value_type='ORG'\n",
    "    target_entity_type='PRODUCT'\n",
    "    \n",
    "    # the following 3 lines merge entities and noun chunks into one token\n",
    "    # this is useful in our cases, so we will always do it.\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "\n",
    "    relations = {}\n",
    "    \n",
    "    manus =utils.get_entities_of_type(property_value_type, doc)\n",
    "    \n",
    "    for manu in manus:\n",
    "        if utils.pattern_found_on_the_left(doc, manu.i, patterns):\n",
    "            prod=utils.find_closest_entity(doc.ents, manu.idx, target_entity_type)\n",
    "            if not prod:\n",
    "                prod = main_entity\n",
    "            if manu and prod:\n",
    "                relations[prod]=manu.text\n",
    "    return relations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1d** Test your *manufacturer substring matching extractor* in the following way. \n",
    "\n",
    "* Write a sentence on which you expect that the extractor *WILL* work. \n",
    "* Write a sentence on which you expect that the extractor *WILL NOT* work. \n",
    "\n",
    "Run your extractor on both sentences and print the results. Make sure that the results are as expected. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sentence \u001b[1m WILL\u001b[0m work: the iPhone was developed by Apple in 2000.\n",
      "{'iPhone': 'Apple'}\n",
      "\n",
      "This sentence \u001b[1m WILL NOT\u001b[0m work: Apple developed the iPad in 2005 .\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "manu_predicates=['manufactured', 'produced', 'developed by', 'developed']\n",
    "main_entity = 'iPhone'\n",
    "sentence='the iPhone was developed by Apple in 2000.'\n",
    "sentence2 = 'Apple developed the iPad in 2005 .'\n",
    "\n",
    "print( \"This sentence \\033[1m WILL\\033[0m work:\",sentence)\n",
    "manu_relations=extract_manufacturer_regex(nlp(sentence), manu_predicates, main_entity)\n",
    "print(manu_relations)\n",
    "print()\n",
    "print( \"This sentence \\033[1m WILL NOT\\033[0m work:\",sentence2)\n",
    "manu_relations2=extract_manufacturer_regex(nlp(sentence2), manu_predicates, main_entity)\n",
    "print(manu_relations2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extracting properties by using dependency information (12 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitting_dependency_year(token, predicates):\n",
    "    \"\"\"\n",
    "    Check whether the we find the right keyword in the correct part of the dependency tree.\n",
    "    \"\"\"\n",
    "    # Find prepositional objects that have a head with dependency label 'agent'\n",
    "    # and its head has a dependency label 'acl'\n",
    "    # Also, we make sure that the head of the head of our object is one of our keywords.\n",
    "    if token.dep_ == 'nsubjpass' and token.head.dep_ == 'ROOT':\n",
    "        pred=token.head.head\n",
    "        if pred.text in predicates:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2a** Write code that extracts the birth year of a person by using dependency information. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_birth_year_dep(doc, predicates):\n",
    "    \n",
    "    property_value_type='PERSON'\n",
    "    target_entity_type='DATE'\n",
    "    \n",
    "    # the following 3 lines merge entities and noun chunks into one token\n",
    "    # this is useful in our cases, so we will always do it.\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "    \n",
    "    relations={}\n",
    "    \n",
    "    # step Ia - generate possible property values\n",
    "    persons=utils.get_entities_of_type(property_value_type, doc)\n",
    "    \n",
    "    for person in persons:\n",
    "        # step Ib - do we find the right keyword in the correct part of the dependency tree?\n",
    "        if fitting_dependency_year(person, predicates):\n",
    "            # step II - find the closest entity of some target type\n",
    "            date=utils.find_closest_entity(doc.ents, person.idx, target_entity_type)\n",
    "            year =utils.extract_year_from_date(date)\n",
    "            if person and year:\n",
    "                relations[person]=year\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_birth_year_dep_entity(doc, predicates,entity):\n",
    "    \n",
    "    property_value_type='PERSON'\n",
    "    target_entity_type='DATE'\n",
    "    \n",
    "    # the following 3 lines merge entities and noun chunks into one token\n",
    "    # this is useful in our cases, so we will always do it.\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "    \n",
    "    relations={}\n",
    "    \n",
    "    # step Ia - generate possible property values\n",
    "    persons=entity\n",
    "            \n",
    "         # step Ia - generate possible property values\n",
    "    birt=utils.get_entities_of_type(property_value_type, doc)\n",
    "    \n",
    "    for person in persons:\n",
    "        # step Ib - do we find the right keyword in the correct part of the dependency tree?\n",
    "        if fitting_dependency_year(person, predicates):\n",
    "            # step II - find the closest entity of some target type\n",
    "            date=utils.find_closest_entity(doc.ents, person.idx, target_entity_type)\n",
    "            print(date)\n",
    "            year =utils.extract_year_from_date(date)\n",
    "            # Devices are often not recognized properly by SpaCy - \n",
    "            # if we find no device, we assume that the relation is about the main entity of the document\n",
    "            if not date:\n",
    "                date=main_entity\n",
    "            if person and year:\n",
    "                relations[person]=year\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2b** Test your *birth year dependency extractor* in the following way. \n",
    "\n",
    "* Write a sentence on which you expect that the extractor *WILL* work. \n",
    "* Write a sentence on which you expect that the extractor *WILL NOT* work. \n",
    "\n",
    "Run your extractor on both sentences and print the results. Make sure that the results are as expected. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sentence \u001b[1m WILL\u001b[0m work: Peter was born in 1975.\n",
      "{Peter: 1975}\n",
      "\n",
      "This sentence \u001b[1m WILL NOT\u001b[0m work: In 1975 Peter's dog was born.\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "born_patterns=['born', 'birthdate']\n",
    "sentence='Peter was born in 1975.'\n",
    "sentence2 = \"In 1975 Peter's dog was born.\"\n",
    "\n",
    "print( \"This sentence \\033[1m WILL\\033[0m work:\",sentence)\n",
    "birth_year_relations=extract_birth_year_dep(nlp(sentence), born_patterns)\n",
    "print(birth_year_relations)\n",
    "print()\n",
    "print( \"This sentence \\033[1m WILL NOT\\033[0m work:\",sentence2)\n",
    "birth_year_relations2=extract_birth_year_dep(nlp(sentence2), born_patterns)\n",
    "print(birth_year_relations2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2c** Write code that extracts the manufacturer of a device by using dependency information. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitting_dependency_manu(token, predicates):\n",
    "    \"\"\"\n",
    "    Check whether the we find the right keyword in the correct part of the dependency tree.\n",
    "    \"\"\"\n",
    "    # Find prepositional objects that have a head with dependency label 'agent'\n",
    "    # and its head has a dependency label 'acl'\n",
    "    # Also, we make sure that the head of the head of our object is one of our keywords.\n",
    "    if token.dep_ == 'pobj' and token.head.dep_ == 'agent' and token.head.head.dep_ =='ROOT':\n",
    "        pred=token.head.head\n",
    "        if pred.text in predicates:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_manufacturer(doc, predicates, main_entity):\n",
    "    \n",
    "    property_value_type='ORG'\n",
    "    target_entity_type='PRODUCT'\n",
    "    \n",
    "    # the following 3 lines merge entities and noun chunks into one token\n",
    "    # this is useful in our cases, so we will always do it.\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "    \n",
    "    relations={}\n",
    "    \n",
    "    # step Ia - generate possible property values\n",
    "    manus=utils.get_entities_of_type(property_value_type, doc)\n",
    "    \n",
    "    for manu in manus:\n",
    "        # step Ib - do we find the right keyword in the correct part of the dependency tree?\n",
    "        if fitting_dependency_manu(manu, predicates):\n",
    "            # step II - find the closest entity of some target type\n",
    "            device=utils.find_closest_entity(doc.ents, manu.idx, target_entity_type)\n",
    "            # Devices are often not recognized properly by SpaCy - \n",
    "            # if we find no device, we assume that the relation is about the main entity of the document\n",
    "            if not device:\n",
    "                device=main_entity\n",
    "            if device and manu:\n",
    "                relations[device]=manu.text\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2d** Test your *manufacturer dependency extractor* in the following way. \n",
    "\n",
    "* Write a sentence on which you expect that the extractor *WILL* work. \n",
    "* Write a sentence on which you expect that the extractor *WILL NOT* work. \n",
    "\n",
    "Run your extractor on both sentences and print the results. Make sure that the results are as expected. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sentence \u001b[1m WILL\u001b[0m work: the iPhone was developed by Apple in 2000.\n",
      "{'iPhone': 'Apple'}\n",
      "\n",
      "This sentence \u001b[1m WILL NOT\u001b[0m work: Apple developed the iPad in 2005.\n",
      "{}\n",
      "This sentence \u001b[1m WILL NOT\u001b[0m work: Walkman is a brand of portable media players manufactured by Sony. The original Walkman, released in 1979, was a portable cassette player that changed listening habits by allowing people to listen to music of their choice on the move. It was devised by Sony founders Masaru Ibuka and Akio Morita, who felt Sony existing portable player was too unwieldy and expensive\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "manu_predicates=['manufactured', 'produced', 'developed']\n",
    "main_entity = 'iPhone'\n",
    "main_entity2 ='Walkman'\n",
    "sentence='the iPhone was developed by Apple in 2000.'\n",
    "sentence2 = 'Apple developed the iPad in 2005.'\n",
    "sentence3= 'Walkman is a brand of portable media players manufactured by Sony. The original Walkman, released in 1979, was a portable cassette player that changed listening habits by allowing people to listen to music of their choice on the move. It was devised by Sony founders Masaru Ibuka and Akio Morita, who felt Sony existing portable player was too unwieldy and expensive'\n",
    "print( \"This sentence \\033[1m WILL\\033[0m work:\",sentence)\n",
    "manu_relations=extract_manufacturer(nlp(sentence), manu_predicates, main_entity)\n",
    "print(manu_relations)\n",
    "\n",
    "print()\n",
    "print( \"This sentence \\033[1m WILL NOT\\033[0m work:\",sentence2)\n",
    "manu_relations2=extract_manufacturer(nlp(sentence2), manu_predicates, main_entity)\n",
    "print(manu_relations2)\n",
    "print( \"This sentence \\033[1m WILL NOT\\033[0m work:\",sentence3)\n",
    "manu_relations3=extract_manufacturer(nlp(sentence2), manu_predicates, main_entity2)\n",
    "print(manu_relations3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Running and evaluating extractors on Wikipedia (8 points)\n",
    "\n",
    "We will run our extractors on 50 documents about people and 50 documents about devices. We provide code to load the lists of entities and the gold values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Al Pacino', 'Alan Rickman', 'Albert Finney', 'Alyson Hannigan', 'Andie MacDowell', 'Andrew Lloyd Webber', 'Andrzej Wajda', 'Andrzej Żuławski', 'Angela Davis', 'Anthony Quinn', 'Antonio Banderas', 'Ashley Judd', 'Ava Gardner', 'Barbara Stanwyck', 'Ben Elton', 'Bernardo Bertolucci', 'Betty Marsden', 'Billy Wilder', 'Blake Edwards', 'Bob Black', 'Bob Keeshan', 'Brad Pitt', 'Cameron Diaz', 'Carmen Miranda', 'Carole Lombard', 'Catherine Deneuve', 'Cesare Zavattini', 'Chandra Levy', 'Charlton Heston', 'Chaz Bono', 'Christine McVie', 'Christopher Lambert', 'Christopher Lee', 'Clark Gable', 'Clint Eastwood', 'Clive Sinclair', 'Cybill Shepherd', 'Dan Aykroyd', 'Dannii Minogue', 'Dave Cutler', 'David Blaine', 'David Boies', 'David Gauthier', 'David Jason', 'David Niven', 'Denise Richards', 'Desmond Llewelyn', 'Don Siegel', 'Dudley Moore', 'Dustin Hoffman']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"birthyears.json\", 'rb') as f:\n",
    "    gold_birthyears=json.load(f)\n",
    "    wiki_people=list(gold_birthyears.keys())\n",
    "    \n",
    "with open(\"manufacturers.json\", 'rb') as f:\n",
    "    gold_manufacturers=json.load(f)\n",
    "    wiki_devices=list(gold_manufacturers.keys())\n",
    "print(wiki_people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lists `wiki_people` and `wiki_devices` contain the names of 50 people and 50 devices, respectively.\n",
    "\n",
    "The dictionaries `gold_birthyears` and `gold_manufacturers` contain gold values for each of these entities.\n",
    "\n",
    "We provide a function that evaluates your extracted property values against known (\"gold\") property values. The function returns three evaluation scores: precision, recall, f1-score. You can find call this function as follows:\n",
    "\n",
    "`utils.evaluate_property(system_json, gold_json)`\n",
    "\n",
    "(make sure to replace the system_json and the gold_json with the concrete dictionaries you are comparing, depending on the property and the method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have stored the gold values for both properties in our dictionaries `gold_birthyears` and `gold_manufacturers`, and written the evaluation function, we need to obtain the system output as well and then perform evaluation.\n",
    "\n",
    "For this purpose, we will run our extractors on texts about the same 50 people and 50 devices from Wikipedia. As in the explanation notebook, we will use the `Wikipedia` library for this purpose. Same as in the explanation notebook, we will only process the first three sentences.\n",
    "\n",
    "In exercises 3a and 3b, we will run all our four processing functions and store the results in four different dictionaries. \n",
    "Then, in exercise 3c, we will run the evaluation function four times to compute precision, recall, and F1-score for all four functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3a** Run your two extractors about birth years of people (from exercise 1a and 1b) on all 50 documents about people. Save the extracted values in two different dictionaries: `birthyear_regex` and `birthyear_dep`. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Al Pacino\n",
      "AlPacino\n",
      "<WikipediaPage 'Al Pacino'>\n"
     ]
    }
   ],
   "source": [
    "al = 'Al Pacino'\n",
    "print(al)\n",
    "al = al.replace(\" \", \"\")\n",
    "print(al)\n",
    "wp = wikipedia.page(al.replace(\" \", \"\"))\n",
    "print(wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_p={}\n",
    "# print(wiki_people)\n",
    "\n",
    "for entity in wiki_people:\n",
    "        try:\n",
    "            a= entity\n",
    "            \n",
    "            wp = wikipedia.page(a.replace(\" \", \"\"))\n",
    "            first_three_sentences=wp.content.split('.')[:3]\n",
    "            entity_text=('.').join(first_three_sentences)\n",
    "            texts_p[entity] = entity_text\n",
    "#             print(entity_text)\n",
    "#             print()\n",
    "        except:\n",
    "            pass\n",
    "#             print(entity, \"was not found on wikipedia\")\n",
    "#             print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Al Pacino': '1940', 'Alyson Hannigan': '1974', 'Andie MacDowell': '1958', 'Andrew Lloyd Webber': '1948', 'Angela Davis': '1944', 'Antonio Banderas': '1960', 'Bob Black': '1951', 'Brad Pitt': '1963', 'Cameron Diaz': '1972', 'Catherine Deneuve': '1943', 'Christine McVie': '1943', 'Clint Eastwood': '1930', 'Cybill Shepherd': '1950', 'Dan Aykroyd': '1952', 'Dave Cutler': '1942', 'David Blaine': '1973', 'David Boies': '1941', 'David Gauthier': '1932', 'Denise Richards': '1971', 'Dustin Hoffman': '1937'}\n",
      "{'Alyson Hannigan': '2009', 'Cybill Shepherd': '2010', 'David Gauthier': '1932', 'Desmond Llewelyn': '1963'}\n"
     ]
    }
   ],
   "source": [
    "birthyear_regex={} \n",
    "birthyear_dep={}\n",
    "\n",
    "# texts_p\n",
    "# print(wiki_people)\n",
    "born_patterns=['born in', 'birthdate', 'born on','(born','born']\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "for entity in wiki_people:\n",
    "    if entity in texts_p.keys():\n",
    "#         print('entity is',entity)\n",
    "#         print('manu_predicates',born_patterns)\n",
    "#         print(nlp(texts_p[entity]))\n",
    "        a=extract_birth_year_regex(nlp(texts_p[entity]), born_patterns)\n",
    "#         print(\"relation  is\",a)\n",
    "        for p in a.values():\n",
    "            birthyear_regex[entity] = str(p)\n",
    " \n",
    "            \n",
    "     \n",
    "for entity in wiki_people:\n",
    "    if entity in texts_p.keys():\n",
    "#         print('entity is',entity)\n",
    "#         print('born_predicates',born_patterns)\n",
    "#         print(nlp(texts_p[entity]))\n",
    "        a=extract_birth_year_dep(nlp(texts_p[entity]), born_patterns)\n",
    "#         print(\"relation  is\",a)\n",
    "#         print()\n",
    "        for p in a.values():\n",
    "            birthyear_dep[entity] = str(p)\n",
    "        \n",
    "    \n",
    "print(birthyear_regex)\n",
    "print(birthyear_dep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3b** Run your extractors about manufacturers of devices (from exercise 2a and 2b) on all 50 documents about devices. Make sure you only process the first three sentences from each document. Save the extracted values in two lists: `manufacturers_regex` and `manufacturers_dep`. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitting_dependency_manu(token, predicates):\n",
    "    \"\"\"\n",
    "    Check whether the we find the right keyword in the correct part of the dependency tree.\n",
    "    \"\"\"\n",
    "    # Find prepositional objects that have a head with dependency label 'agent'\n",
    "    # and its head has a dependency label 'acl'\n",
    "    # Also, we make sure that the head of the head of our object is one of our keywords.\n",
    "    if token.dep_ == 'pobj' and token.head.dep_ == 'agent' and token.head.head.dep_ =='acl':\n",
    "        pred=token.head.head\n",
    "        if pred.text in predicates:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pleun/anaconda3/lib/python3.7/site-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/pleun/anaconda3/lib/python3.7/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "textsdev={}\n",
    "wiki_devices\n",
    "for entity in wiki_devices:\n",
    "    try:    \n",
    "#         print(entity)\n",
    "        a=entity\n",
    "        wp = wikipedia.page(a.replace(\" \", \"\"))\n",
    "        # get the first 3 sentences of a wikipedia article\n",
    "        first_three_sentences=wp.content.split('.')[:3]\n",
    "        entity_text=('.').join(first_three_sentences)\n",
    "        # create a dictionary (JSON) where the key is your entity, and the value is its 3-sentences wikipedia text. \n",
    "        textsdev[entity]=entity_text\n",
    "#         print(entity_text)\n",
    "#         print()\n",
    "    except:\n",
    "        pass\n",
    "#         print(entity, \"was not found on wikipedia\")\n",
    "#         print()\n",
    "\n",
    "    \n",
    "\n",
    "# print(textsdev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PDP-7': 'Digital Equipment Corporation', 'The original Walkman': 'Sony', 'Sega TeraDrive': 'IBM', 'PlayStation 2': 'Nintendo', 'Cray-1': 'Cray Research', 'TRS-80': 'Tandy Corporation', 'Vectrex': 'General Computer Electronics', \"Sinclair's ZX80\": 'Timex Corporation', 'Volkswagen': 'Volkswagen Group', 'Spice Stellar Nhance Mi-435': 'Spice mobile Corporation', 'IPod Mini': 'Apple Inc.', 'Mac Mini': 'Apple Inc. It', 'Coleco Gemini': 'Coleco Industries, Inc.', 'Zune HD': 'Microsoft', 'Zune 30': 'Microsoft', 'Zune 80, 120': 'Microsoft', 'Motorola W233': 'T-Mobile USA', 'Motoblur': 'Motorola', 'Motorola A3100': 'Motorola', 'Motorola Calgary': 'Verizon Wireless', 'Motorola Photon Q': 'Motorola', 'Game Boy': 'the Game & Watch', 'Nokia 6210 Navigator': 'Nokia', 'Nokia 6710 Navigator': 'Nokia'}\n",
      "{'PDP-7': 'Digital Equipment Corporation', 'The original Walkman': 'Sony', 'Vectrex': 'Smith Engineering', 'Volkswagen': 'Volkswagen Group', 'Coleco Gemini': 'Coleco Industries, Inc.', 'Zune 80, 120': 'Microsoft', 'Motorola Photon Q': 'Motorola'}\n"
     ]
    }
   ],
   "source": [
    "manufacturers_regex={}\n",
    "manufacturers_dep={}\n",
    "# textsdev\n",
    "# print(wiki_devices)\n",
    "manu_predicates=['manufactured', 'produced', 'developed','designed','by',' developed by','produced by','released by','made by','marketed by']\n",
    "\n",
    "for entity in wiki_devices: \n",
    "    if entity in textsdev.keys():\n",
    "#         print('entity is',entity)\n",
    "#         print('manu_predicates',manu_predicates)\n",
    "#          print(nlp(textsdev[entity]))\n",
    "         manu_relations=extract_manufacturer(nlp(textsdev[entity]), manu_predicates, entity)\n",
    "#         print(\"relation  is\",manu_relations)\n",
    "#         print()\n",
    "         manufacturers_dep.update(manu_relations)\n",
    "#         manufacturers_dep[entity]=manu_relation[1]\n",
    "    \n",
    "    \n",
    "    \n",
    "for entity in wiki_devices:\n",
    "    if entity in textsdev.keys():\n",
    "#         print('entity is',entity)\n",
    "#         print('manu_predicates',manu_predicates)\n",
    "#         print(nlp(textsdev[entity]))\n",
    "        manu_relations=extract_manufacturer_regex(nlp(textsdev[entity]), manu_predicates, entity)\n",
    "#         print(\"relation  is\",manu_relations)\n",
    "#         print()\n",
    "        manufacturers_regex.update(manu_relations)\n",
    "       #manufacturers_regex[entity]=manu_relation[1]\n",
    "\n",
    "    \n",
    "print(manufacturers_regex)\n",
    "print(manufacturers_dep)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3c** Run the evaluation function `evaluate_property` to compute the performance for each of your four functions. Print the precision, recall, and F1-scores. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manufacturers_regex:\n",
      "precision: 0.631579, \n",
      "recall: 0.250000, \n",
      "F1-score: 0.358209\n",
      "\n",
      "Manufacturers_dependency:\n",
      "precision: 0.600000, \n",
      "recall: 0.062500, \n",
      "F1-score: 0.113208\n",
      "\n",
      "birthyear_regex:\n",
      "precision: 1.000000, \n",
      "recall: 0.400000, \n",
      "F1-score: 0.571429\n",
      "\n",
      "birthyear_dependency:\n",
      "precision: 0.250000, \n",
      "recall: 0.020000, \n",
      "F1-score: 0.037037\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(utils.evaluate_property(manufacturers_regex, gold_manufacturers))\n",
    "print('Manufacturers_regex:')\n",
    "precision, recall, f1=utils.evaluate_property(manufacturers_regex, gold_manufacturers)\n",
    "print(\"precision: %f, \\nrecall: %f, \\nF1-score: %f\\n\" % (precision, recall, f1))\n",
    "\n",
    "# print(utils.evaluate_property(manufacturers_dep, gold_manufacturers))\n",
    "print('Manufacturers_dependency:')\n",
    "precision, recall, f1=utils.evaluate_property(manufacturers_dep, gold_manufacturers)\n",
    "print(\"precision: %f, \\nrecall: %f, \\nF1-score: %f\\n\" % (precision, recall, f1))\n",
    "\n",
    "# print(utils.evaluate_property(birthyear_regex, gold_birthyears))\n",
    "print('birthyear_regex:')\n",
    "precision, recall, f1=utils.evaluate_property(birthyear_regex, gold_birthyears)\n",
    "print(\"precision: %f, \\nrecall: %f, \\nF1-score: %f\\n\" % (precision, recall, f1))\n",
    "\n",
    "# print(utils.evaluate_property(birthyear_dep, gold_birthyears))\n",
    "print('birthyear_dependency:')\n",
    "precision, recall, f1=utils.evaluate_property(birthyear_dep, gold_birthyears)\n",
    "print(\"precision: %f, \\nrecall: %f, \\nF1-score: %f\\n\" % (precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Reflection (8 points)\n",
    "\n",
    "For each entity, we will now compare the two methods to extract properties in terms of precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4a** Comparing the precision between the methods based on regular expressions and on syntax dependencies:\n",
    "* Which method yields lower precision?\n",
    "* Why do you think this is the case?\n",
    "* Give an example to support your argument.\n",
    "\n",
    "(4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4b** Let's compare the recall for both properties. \n",
    "* Which method yields lower recall?\n",
    "* Why do you think this is the case?\n",
    "* Give an example to support your argument.\n",
    "\n",
    "(4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
